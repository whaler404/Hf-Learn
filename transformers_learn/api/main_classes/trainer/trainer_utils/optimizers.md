# æ”¯æŒçš„ä¼˜åŒ–å™¨

`OptimizerNames` æšä¸¾ç±»å­˜å‚¨äº† HuggingFace Transformers ä¸­æ‰€æœ‰å¯ç”¨çš„ä¼˜åŒ–å™¨æ ‡è¯†ç¬¦ã€‚è¿™äº›ä¼˜åŒ–å™¨æ¶µç›–äº†ä»ç»å…¸çš„ AdamW åˆ°æœ€æ–°çš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨ã€‚

## æ¦‚è¿°

ä¼˜åŒ–å™¨æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£æ ¹æ®è®¡ç®—å‡ºçš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚Transformers åº“æä¾›äº†ä¸°å¯Œçš„ä¼˜åŒ–å™¨é€‰æ‹©ï¼Œé€‚ç”¨äºä¸åŒçš„è®­ç»ƒåœºæ™¯å’Œç¡¬ä»¶æ¡ä»¶ã€‚

## ä¼˜åŒ–å™¨åˆ†ç±»

### ğŸ”¥ AdamW ç³»åˆ—ä¼˜åŒ–å™¨

#### æ ‡å‡†å®ç°
| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **PyTorch AdamW** | `"adamw_torch"` | PyTorch åŸç”Ÿ AdamW å®ç° | ç¨³å®šå¯é ï¼Œå…¼å®¹æ€§æœ€å¥½ |
| **PyTorch Fused AdamW** | `"adamw_torch_fused"` | PyTorch èåˆç‰ˆæœ¬çš„ AdamW | æ›´å¿«çš„è®¡ç®—é€Ÿåº¦ï¼ŒCUDA ä¼˜åŒ– |
| **PyTorch XLA AdamW** | `"adamw_torch_xla"` | æ”¯æŒ TPU çš„ AdamW | é€‚ç”¨äº TPU è®­ç»ƒ |
| **NPU Fused AdamW** | `"adamw_torch_npu_fused"` | æ”¯æŒ NPU çš„èåˆ AdamW | é€‚ç”¨äºåä¸º NPU |

#### é«˜ç²¾åº¦å’Œå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **Apex Fused AdamW** | `"adamw_apex_fused"` | NVIDIA Apex èåˆç‰ˆæœ¬ | Apex åº“æä¾›ï¼Œæ€§èƒ½ä¼˜å¼‚ |
| **AnyPrecision AdamW** | `"adamw_anyprecision"` | æ”¯æŒä»»æ„ç²¾åº¦çš„ AdamW | å¯è‡ªå®šä¹‰ä¸åŒéƒ¨åˆ†çš„ç²¾åº¦ |
| **AdamW 4-bit** | `"adamw_torch_4bit"` | 4-bit é‡åŒ–ç‰ˆæœ¬ | æå¤§å‡å°‘å†…å­˜å ç”¨ |
| **AdamW 8-bit** | `"adamw_torch_8bit"` | 8-bit é‡åŒ–ç‰ˆæœ¬ | å¹³è¡¡å†…å­˜å’Œç²¾åº¦ |
| **BitsAndBytes 8-bit AdamW** | `"adamw_bnb_8bit"` | BnB å®ç°çš„ 8-bit AdamW | ä¸ BnB ç”Ÿæ€é›†æˆ |

### ğŸ¯ Lion ä¼˜åŒ–å™¨ç³»åˆ—

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **Lion 32-bit** | `"lion_32bit"` | æ ‡å‡†ç²¾åº¦ Lion ä¼˜åŒ–å™¨ | å†…å­˜æ•ˆç‡é«˜ï¼Œæ€§èƒ½å¥½ |
| **Lion 8-bit** | `"lion_8bit"` | 8-bit Lion ä¼˜åŒ–å™¨ | è¿›ä¸€æ­¥èŠ‚çœå†…å­˜ |
| **Paged Lion 32-bit** | `"paged_lion_32bit"` | åˆ†é¡µç‰ˆæœ¬çš„ Lion | å¤§æ¨¡å‹è®­ç»ƒå‹å¥½ |
| **Paged Lion 8-bit** | `"paged_lion_8bit"` | åˆ†é¡µ 8-bit Lion | ç»“åˆå†…å­˜å’Œæ€§èƒ½ä¼˜åŒ– |

### ğŸ¯ AdEMAMix ä¼˜åŒ–å™¨ç³»åˆ—

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **AdEMAMix** | `"ademamix"` | æ··åˆåŠ¨é‡ä¼˜åŒ–å™¨ | ç»“åˆå¤šç§åŠ¨é‡ç­–ç•¥ |
| **AdEMAMix 8-bit** | `"ademamix_8bit"` | 8-bit ç‰ˆæœ¬ | å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ |
| **Paged AdEMAMix 32-bit** | `"paged_ademamix_32bit"` | åˆ†é¡µç‰ˆæœ¬ | å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ– |
| **Paged AdEMAMix 8-bit** | `"paged_ademamix_8bit"` | åˆ†é¡µ 8-bit ç‰ˆæœ¬ | ç»¼åˆä¼˜åŒ–ç‰ˆæœ¬ |

### ğŸ“„ Paged ä¼˜åŒ–å™¨ç³»åˆ—

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **Paged AdamW 32-bit** | `"paged_adamw_32bit"` | åˆ†é¡µç‰ˆæœ¬ AdamW | é€‚åˆå¤§æ¨¡å‹ï¼Œé¿å… OOM |
| **Paged AdamW 8-bit** | `"paged_adamw_8bit"` | åˆ†é¡µ 8-bit AdamW | å†…å­˜é«˜æ•ˆçš„å¤§æ¨¡å‹è®­ç»ƒ |

### ğŸŒŸ GaLore (Gradient Low-Rank Projection) ä¼˜åŒ–å™¨

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **GaLore AdamW** | `"galore_adamw"` | æ¢¯åº¦ä½ç§©æŠ•å½± AdamW | å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œä¿æŒæ€§èƒ½ |
| **GaLore AdamW 8-bit** | `"galore_adamw_8bit"` | 8-bit GaLore AdamW | è¿›ä¸€æ­¥å†…å­˜ä¼˜åŒ– |
| **GaLore Adafactor** | `"galore_adafactor"` | GaLore + Adafactor | åŒé‡å†…å­˜ä¼˜åŒ– |
| **GaLore AdamW Layerwise** | `"galore_adamw_layerwise"` | é€å±‚ GaLore AdamW | æ›´ç²¾ç»†çš„å†…å­˜æ§åˆ¶ |
| **GaLore AdamW 8-bit Layerwise** | `"galore_adamw_8bit_layerwise"` | é€å±‚ 8-bit GaLore | æœ€ä¼˜å†…å­˜æ•ˆç‡ |
| **GaLore Adafactor Layerwise** | `"galore_adafactor_layerwise"` | é€å±‚ GaLore Adafactor | å†…å­˜ä¼˜åŒ–é€å±‚ç‰ˆæœ¬ |

### ğŸš€ APOLLO ä¼˜åŒ–å™¨ç³»åˆ—

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **APOLLO AdamW** | `"apollo_adamw"` | APOLLO ä¼˜åŒ–å™¨ | é«˜æ•ˆçš„äºŒé˜¶ä¼˜åŒ–æ–¹æ³• |
| **APOLLO AdamW Layerwise** | `"apollo_adamw_layerwise"` | é€å±‚ APOLLO | æ›´ç²¾ç¡®çš„ä¼˜åŒ–æ§åˆ¶ |

### ğŸ”¬ ä¼ ç»Ÿä¼˜åŒ–å™¨

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **SGD** | `"sgd"` | éšæœºæ¢¯åº¦ä¸‹é™ | ç®€å•ç»å…¸ï¼Œé€‚åˆç®€å•ä»»åŠ¡ |
| **Adagrad** | `"adagrad"` | è‡ªé€‚åº”æ¢¯åº¦ç®—æ³• | é€‚åˆç¨€ç–æ•°æ® |
| **Adafactor** | `"adafactor"` | å†…å­˜é«˜æ•ˆçš„è‡ªé€‚åº”ä¼˜åŒ–å™¨ | Google å¼€å‘ï¼Œé€‚åˆå¤§æ¨¡å‹ |
| **RMSprop** | `"rmsprop"` | å‡æ–¹æ ¹ä¼ æ’­ | é€‚åˆ RNN å’Œè¯­éŸ³ä»»åŠ¡ |
| **RMSprop BnB** | `"rmsprop_bnb"` | BitsAndBytes RMSprop | å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ |
| **RMSprop 8-bit** | `"rmsprop_bnb_8bit"` | 8-bit RMSprop | æç®€å†…å­˜å ç”¨ |
| **RMSprop 32-bit** | `"rmsprop_bnb_32bit"` | 32-bit BnB RMSprop | æ ‡å‡†ç²¾åº¦ BnB ç‰ˆæœ¬ |

### ğŸ§ª å®éªŒæ€§å’Œä¸“ç”¨ä¼˜åŒ–å™¨

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **LoMo** | `"lomo"` | ä½å†…å­˜ä¼˜åŒ–å™¨ | ä¸“ä¸ºå†…å­˜å—é™ç¯å¢ƒè®¾è®¡ |
| **AdaLoMo** | `"adalomo"` | è‡ªé€‚åº” LoMo | æ™ºèƒ½å†…å­˜ç®¡ç† |
| **GrokAdamW** | `"grokadamw"` | Grok ä¼˜åŒ–å™¨å˜ç§ | åŸºäºæœ€æ–°ç ”ç©¶æˆæœ |
| **Stable AdamW** | `"stable_adamw"` | ç¨³å®šç‰ˆ AdamW | æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ– |

### â° Schedule-Free ä¼˜åŒ–å™¨ç³»åˆ—

| ä¼˜åŒ–å™¨åç§° | æ ‡è¯†ç¬¦ | æè¿° | ç‰¹ç‚¹ |
|------------|--------|------|------|
| **Schedule-Free RAdam** | `"schedule_free_radam"` | æ— è°ƒåº¦å™¨ RAdam | å†…ç½®å­¦ä¹ ç‡è°ƒåº¦ |
| **Schedule-Free AdamW** | `"schedule_free_adamw"` | æ— è°ƒåº¦å™¨ AdamW | ç®€åŒ–è®­ç»ƒé…ç½® |
| **Schedule-Free SGD** | `"schedule_free_sgd"` | æ— è°ƒåº¦å™¨ SGD | è‡ªåŠ¨å­¦ä¹ ç‡è°ƒæ•´ |

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from transformers import TrainingArguments, Trainer

# ä½¿ç”¨é»˜è®¤çš„ AdamW ä¼˜åŒ–å™¨
training_args = TrainingArguments(
    output_dir="./results",
    optim="adamw_torch",  # é»˜è®¤å€¼
    learning_rate=5e-5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)
```

### å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨

```python
# 8-bit é‡åŒ–ä¼˜åŒ–å™¨ï¼ˆé€‚åˆå¤§æ¨¡å‹ï¼‰
training_args = TrainingArguments(
    output_dir="./results",
    optim="adamw_8bit",  # 8-bit AdamW
    learning_rate=5e-5,
    weight_decay=0.01,
)

# GaLore ä¼˜åŒ–å™¨ï¼ˆæ¢¯åº¦ä½ç§©æŠ•å½±ï¼‰
training_args = TrainingArguments(
    output_dir="./results",
    optim="galore_adamw",  # éœ€è¦ galore_torch åŒ…
    learning_rate=5e-5,
    optim_target_modules=["q_proj", "v_proj", "k_proj"],  # ç›®æ ‡æ¨¡å—
)
```

### é«˜æ€§èƒ½ä¼˜åŒ–å™¨

```python
# èåˆç‰ˆæœ¬ä¼˜åŒ–å™¨ï¼ˆCUDA ä¼˜åŒ–ï¼‰
training_args = TrainingArguments(
    output_dir="./results",
    optim="adamw_torch_fused",  # èåˆ AdamW
    learning_rate=5e-5,
    fp16=True,  # é…åˆæ··åˆç²¾åº¦
)

# Lion ä¼˜åŒ–å™¨ï¼ˆæ–°æ¶æ„ï¼‰
training_args = TrainingArguments(
    output_dir="./results",
    optim="lion_32bit",  # Lion ä¼˜åŒ–å™¨
    learning_rate=1e-4,  # Lion é€šå¸¸éœ€è¦æ›´é«˜å­¦ä¹ ç‡
    weight_decay=0.1,
)
```

### åˆ†é¡µä¼˜åŒ–å™¨ï¼ˆå¤§æ¨¡å‹è®­ç»ƒï¼‰

```python
# åˆ†é¡µä¼˜åŒ–å™¨ï¼ˆé¿å… OOMï¼‰
training_args = TrainingArguments(
    output_dir="./results",
    optim="paged_adamw_8bit",  # åˆ†é¡µ 8-bit AdamW
    learning_rate=5e-5,
    per_device_train_batch_size=1,  # å°æ‰¹æ¬¡
    gradient_accumulation_steps=32,  # æ¢¯åº¦ç´¯ç§¯
)
```

## ä¾èµ–è¦æ±‚

ä¸åŒä¼˜åŒ–å™¨å¯èƒ½éœ€è¦å®‰è£…é¢å¤–çš„åŒ…ï¼š

```bash
# 8-bit ä¼˜åŒ–å™¨
pip install bitsandbytes

# GaLore ä¼˜åŒ–å™¨
pip install git+https://github.com/jiaweizzhao/GaLore

# APOLLO ä¼˜åŒ–å™¨
pip install apollo-torch

# AnyPrecision ä¼˜åŒ–å™¨
pip install git+https://github.com/pytorch/torchdistx

# GrokAdamW ä¼˜åŒ–å™¨
pip install torch-optimi
```

## é€‰æ‹©æŒ‡å—

### æ ¹æ®æ¨¡å‹å¤§å°é€‰æ‹©
- **å°æ¨¡å‹ (< 1B å‚æ•°)**: `adamw_torch`, `adamw_torch_fused`
- **ä¸­ç­‰æ¨¡å‹ (1B-7B å‚æ•°)**: `adamw_8bit`, `lion_32bit`
- **å¤§æ¨¡å‹ (7B+ å‚æ•°)**: `galore_adamw`, `paged_adamw_8bit`

### æ ¹æ®ç¡¬ä»¶æ¡ä»¶é€‰æ‹©
- **å……è¶³ GPU å†…å­˜**: `adamw_torch_fused`, `adamw_apex_fused`
- **æœ‰é™ GPU å†…å­˜**: `adamw_8bit`, `lion_8bit`
- **æåº¦å†…å­˜å—é™**: `galore_adamw_8bit_layerwise`, `lomo`

### æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©
- **é€šç”¨ä»»åŠ¡**: `adamw_torch`, `adamw_torch_fused`
- **å¤§è¯­è¨€æ¨¡å‹**: `galore_adamw`, `adamw_8bit`
- **è®¡ç®—æœºè§†è§‰**: `adamw_torch`, `lion_32bit`
- **æ¨èç³»ç»Ÿ/ç¨€ç–æ•°æ®**: `adagrad`

## æ€§èƒ½å¯¹æ¯”

### å†…å­˜ä½¿ç”¨ï¼ˆä»ä½åˆ°é«˜ï¼‰
1. `galore_adamw_8bit_layerwise` - æœ€ä½
2. `adamw_8bit`, `lion_8bit`
3. `galore_adamw`, `paged_adamw_8bit`
4. `adamw_torch_8bit`
5. `lion_32bit`, `ademamix`
6. `adamw_torch`, `adamw_torch_fused`

### è®­ç»ƒé€Ÿåº¦ï¼ˆä»å¿«åˆ°æ…¢ï¼‰
1. `adamw_torch_fused`, `adamw_apex_fused` - æœ€å¿«
2. `adamw_torch`
3. `lion_32bit`, `lion_8bit`
4. `adamw_8bit`
5. `galore_adamw`
6. `ademamix`, `apollo_adamw`

### æ”¶æ•›æ€§èƒ½ï¼ˆä¸€èˆ¬æ’åºï¼‰
1. `adamw_torch`, `adamw_torch_fused` - æœ€ç¨³å®š
2. `ademamix` - æ”¶æ•›æ€§å¥½
3. `galore_adamw` - å†…å­˜æ•ˆç‡é«˜ä¸”æ€§èƒ½å¥½
4. `lion_32bit` - æ–°æ¶æ„ï¼Œè¡¨ç°ä¼˜å¼‚
5. `adamw_8bit` - ç•¥æœ‰ç²¾åº¦æŸå¤±

## æœ€ä½³å®è·µ

1. **é»˜è®¤é€‰æ‹©**: ä» `adamw_torch` å¼€å§‹ï¼Œè¿™æ˜¯æœ€ç¨³å®šçš„é€‰æ‹©
2. **å†…å­˜ä¸è¶³**: å°è¯• `adamw_8bit` æˆ– `galore_adamw`
3. **è¿½æ±‚é€Ÿåº¦**: ä½¿ç”¨ `adamw_torch_fused`ï¼ˆå¦‚æœæœ‰ CUDAï¼‰
4. **å¤§æ¨¡å‹è®­ç»ƒ**: ä¼˜å…ˆè€ƒè™‘ `galore_adamw` ç³»åˆ—æˆ– `paged_adamw_8bit`
5. **å®éªŒæ–°æ–¹æ³•**: å¯ä»¥å°è¯• `lion_32bit` æˆ– `ademamix`

## æ³¨æ„äº‹é¡¹

- 8-bit ä¼˜åŒ–å™¨éœ€è¦ `bitsandbytes >= 0.41.1` ç‰ˆæœ¬ä»¥é¿å…å·²çŸ¥ bug
- GaLore å’Œ APOLLO ä¼˜åŒ–å™¨éœ€è¦é¢å¤–å®‰è£…ç›¸åº”åŒ…
- åˆ†é¡µä¼˜åŒ–å™¨é€‚åˆå¤§æ¨¡å‹ä½†å¯èƒ½æœ‰è½»å¾®æ€§èƒ½å¼€é”€
- æŸäº›ä¼˜åŒ–å™¨å¯¹å­¦ä¹ ç‡æ•æ„Ÿï¼Œéœ€è¦ç›¸åº”è°ƒæ•´
- æ··åˆç²¾åº¦è®­ç»ƒä¸é‡åŒ–ä¼˜åŒ–å™¨é…åˆä½¿ç”¨æ•ˆæœæœ€ä½³