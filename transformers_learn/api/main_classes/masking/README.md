# æ³¨æ„åŠ›æ©ç ç³»ç»Ÿæ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» Transformers åº“ä¸­æ³¨æ„åŠ›æ©ç çš„è®¾è®¡ã€å®ç°å’Œä½¿ç”¨æ–¹æ³•ã€‚æ³¨æ„åŠ›æ©ç æ˜¯ Transformer æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œæ§åˆ¶æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¿¡æ¯è®¿é—®æ¨¡å¼ã€‚

## ğŸ“š æ–‡æ¡£ç»“æ„

### ğŸ—ï¸ [åŸºç¡€æ©ç å‡½æ•°](./basic_mask_functions.md)
- **å› æœæ©ç å‡½æ•°** (`causal_mask_function`): åŸºç¡€çš„è‡ªå›å½’æ©ç 
- **æ»‘åŠ¨çª—å£å åŠ ** (`sliding_window_overlay`): å±€éƒ¨æ³¨æ„åŠ›æ¨¡å¼
- **åˆ†å—å åŠ ** (`chunked_overlay`): åˆ†å—æ³¨æ„åŠ›æ¨¡å¼
- **å¡«å……æ©ç å‡½æ•°** (`padding_mask_function`): å¤„ç†åºåˆ—å¡«å……
- **æ‰“åŒ…åºåˆ—æ©ç å‡½æ•°** (`packed_sequence_mask_function`): å¤„ç†æ‰“åŒ…åºåˆ—

### ğŸ”§ [æ©ç ç»„åˆå‡½æ•°](./mask_composition.md)
- **AND æ©ç ç»„åˆ** (`and_masks`): å¤šä¸ªæ©ç çš„äº¤é›†
- **OR æ©ç ç»„åˆ** (`or_masks`): å¤šä¸ªæ©ç çš„å¹¶é›†
- **æ©ç åç§»å‡½æ•°** (`add_offsets_to_mask_function`): å¤„ç†ç¼“å­˜åç§»
- å®é™…åº”ç”¨ç¤ºä¾‹å’Œæ€§èƒ½ä¼˜åŒ–æŠ€å·§

### âš¡ [æ³¨æ„åŠ›å®ç°æ©ç ](./attention_implementations.md)
- **SDPA æ©ç ** (`sdpa_mask`): PyTorch æ ‡å‡†æ³¨æ„åŠ›å®ç°
- **Eager æ©ç ** (`eager_mask`): æ˜¾å¼æ³¨æ„åŠ›è®¡ç®—æ©ç 
- **Flash Attention æ©ç ** (`flash_attention_mask`): é«˜æ€§èƒ½ Flash Attention
- **Flex Attention æ©ç ** (`flex_attention_mask`): çµæ´»çš„å—çº§æ©ç 
- æ©ç æ¥å£é€‰æ‹©å’Œæ€§èƒ½å¯¹æ¯”

### ğŸ¯ [é«˜çº§æ©ç åˆ›å»º](./high_level_mask_creation.md)
- **å› æœæ©ç åˆ›å»º** (`create_causal_mask`): å®Œæ•´å› æœæ©ç åˆ›å»º
- **æ»‘åŠ¨çª—å£å› æœæ©ç åˆ›å»º** (`create_sliding_window_causal_mask`): æ»‘åŠ¨çª—å£å®ç°
- **åˆ†å—å› æœæ©ç åˆ›å»º** (`create_chunked_causal_mask`): åˆ†å—æ³¨æ„åŠ›å®ç°
- **ç”Ÿæˆæ©ç åˆ›å»º** (`create_masks_for_generate`): ç”Ÿæˆåœºæ™¯çš„æ©ç å¤„ç†
- é¢„å¤„ç†å™¨å‡½æ•°å’Œæ··åˆå±‚æ”¯æŒ

### ğŸ“Š [ä½¿ç”¨ç¤ºä¾‹ä¸å¯è§†åŒ–](./examples_and_visualizations.md)
- åŸºç¡€ä½¿ç”¨ç¤ºä¾‹å’Œä»£ç æ¼”ç¤º
- è¯¦ç»†çš„å¼ é‡å½¢çŠ¶å˜åŒ–å›¾è§£
- æ©ç å¯è§†åŒ–å·¥å…·å’Œæ¯”è¾ƒæ–¹æ³•
- å®é™…åº”ç”¨åœºæ™¯ï¼ˆæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ç³»ç»Ÿã€å¤šæ¨¡æ€ï¼‰
- è°ƒè¯•æŠ€å·§å’Œæ€§èƒ½åˆ†æ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€å› æœæ©ç 

```python
import torch
from transformers import AutoConfig
from transformers.utils.masking_utils import create_causal_mask

# è®¾ç½®åŸºç¡€å‚æ•°
config = AutoConfig.from_pretrained("gpt2")
batch_size = 2
seq_length = 5
hidden_dim = 768

# åˆ›å»ºè¾“å…¥
input_embeds = torch.randn(batch_size, seq_length, hidden_dim)
cache_position = torch.arange(seq_length)

# åˆ›å»ºå› æœæ©ç 
causal_mask = create_causal_mask(
    config=config,
    input_embeds=input_embeds,
    attention_mask=None,
    cache_position=cache_position,
    past_key_values=None
)

print(f"æ©ç å½¢çŠ¶: {causal_mask.shape}")  # [2, 1, 5, 5]
```

### æ»‘åŠ¨çª—å£æ©ç 

```python
# é…ç½®æ»‘åŠ¨çª—å£
config.sliding_window = 3

# åˆ›å»ºæ»‘åŠ¨çª—å£æ©ç 
sliding_mask = create_sliding_window_causal_mask(
    config=config,
    input_embeds=input_embeds,
    attention_mask=None,
    cache_position=cache_position,
    past_key_values=None
)
```

### åˆ†å—æ³¨æ„åŠ›æ©ç 

```python
# é…ç½®åˆ†å—å¤§å°
config.attention_chunk_size = 4

# åˆ›å»ºåˆ†å—æ©ç 
chunked_mask = create_chunked_causal_mask(
    config=config,
    input_embeds=input_embeds,
    attention_mask=None,
    cache_position=cache_position,
    past_key_values=None
)
```

## ğŸ¨ æ©ç æ¨¡å¼å¯è§†åŒ–

### å› æœæ©ç  (Causal Mask)
```
q\kv 0 1 2 3 4
  0  â–  â¬š â¬š â¬š â¬š
  1  â–  â–  â¬š â¬š â¬š
  2  â–  â–  â–  â¬š â¬š
  3  â–  â–  â–  â–  â¬š
  4  â–  â–  â–  â–  â– 
```

### æ»‘åŠ¨çª—å£ (Sliding Window, window=3)
```
q\kv 0 1 2 3 4
  0  â–  â¬š â¬š â¬š â¬š
  1  â–  â–  â¬š â¬š â¬š
  2  â–  â–  â–  â¬š â¬š
  3  â¬š â–  â–  â–  â¬š
  4  â¬š â¬š â–  â–  â– 
```

### åˆ†å—æ³¨æ„åŠ› (Chunked Attention, chunk_size=3)
```
q\kv 0 1 2 3 4 5
  0  â–  â–  â–  â¬š â¬š â¬š
  1  â–  â–  â–  â¬š â¬š â¬š
  2  â–  â–  â–  â¬š â¬š â¬š
  3  â¬š â¬š â¬š â–  â–  â– 
  4  â¬š â¬š â¬š â–  â–  â– 
  5  â¬š â¬š â¬š â–  â–  â– 
```

## ğŸ” æ ¸å¿ƒæ¦‚å¿µ

### æ©ç å‡½æ•°ç­¾å
æ‰€æœ‰åŸºç¡€æ©ç å‡½æ•°éƒ½éµå¾ªç»Ÿä¸€çš„æ¥å£ï¼š
```python
def mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool
```

### å¼ é‡å½¢çŠ¶å˜æ¢
```
è¾“å…¥: æ ‡é‡ç´¢å¼• (batch_idx, head_idx, q_idx, kv_idx)
     â†“ vmap æ‰©å±•
è¾“å‡º: 4D æ©ç å¼ é‡ (batch_size, num_heads, seq_len, seq_len)
```

### æ³¨æ„åŠ›åç«¯é€‰æ‹©
```python
# è‡ªåŠ¨æ ¹æ®é…ç½®é€‰æ‹©æœ€ä¼˜åç«¯
mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]

# å¯ç”¨åç«¯:
# - "sdpa": PyTorch æ ‡å‡†æ³¨æ„åŠ› (æ¨è)
# - "eager": æ˜¾å¼è®¡ç®— (è°ƒè¯•ç”¨)
# - "flash_attention_2": é«˜æ€§èƒ½ Flash Attention
# - "flex_attention": çµæ´»å—çº§æ³¨æ„åŠ›
```

## ğŸ¯ åº”ç”¨åœºæ™¯

### æ–‡æœ¬ç”Ÿæˆ
- **ç¼–ç é˜¶æ®µ**: å®Œå…¨å› æœæ©ç 
- **ç”Ÿæˆé˜¶æ®µ**: å¢é‡æ©ç  + ç¼“å­˜ä¼˜åŒ–

### é•¿åºåˆ—å¤„ç†
- **æ»‘åŠ¨çª—å£**: Mistral é£æ ¼çš„å±€éƒ¨æ³¨æ„åŠ›
- **åˆ†å—æ³¨æ„åŠ›**: Llama4 é£æ ¼çš„å—çŠ¶æ³¨æ„åŠ›

### å¤šæ¨¡æ€æ¨¡å‹
- **å›¾æ–‡ç†è§£**: å›¾åƒå…¨å±€ + æ–‡æœ¬å› æœ
- **å¯¹è¯ç³»ç»Ÿ**: è§’è‰²åŸºç¡€çš„æ³¨æ„åŠ›è§„åˆ™

### ç‰¹æ®Šåº”ç”¨
- **æ‰“åŒ…åºåˆ—**: å¤šåºåˆ—å¹¶è¡Œå¤„ç†
- **æ··åˆæ¶æ„**: ä¸åŒå±‚ä½¿ç”¨ä¸åŒæ³¨æ„åŠ›æ¨¡å¼

## âš¡ æ€§èƒ½ä¼˜åŒ–

### æ©ç è·³è¿‡ä¼˜åŒ–
```python
# å…è®¸åœ¨ç®€å•åœºæ™¯ä¸‹è·³è¿‡æ©ç åˆ›å»º
mask = sdpa_mask(
    batch_size=batch_size,
    cache_position=cache_position,
    kv_length=kv_length,
    allow_is_causal_skip=True  # å…³é”®ä¼˜åŒ–
)
```

### å†…å­˜ä¼˜åŒ–
- **BlockMask**: Flex Attention çš„å‹ç¼©è¡¨ç¤º
- **ç¨€ç–å­˜å‚¨**: é¿å…ç¨ å¯†å¼ é‡å­˜å‚¨
- **è®¾å¤‡å¯¹é½**: å‡å°‘ GPU-CPU æ•°æ®ä¼ è¾“

### ç¼–è¯‘ä¼˜åŒ–
- **Torch ç¼–è¯‘**: é™æ€æ©ç é¢„ç¼–è¯‘
- **JIT è¿½è¸ª**: åŠ¨æ€æ©ç çš„å³æ—¶ç¼–è¯‘

## ğŸ› ï¸ è°ƒè¯•ä¸éªŒè¯

### æ©ç æ­£ç¡®æ€§æ£€æŸ¥
```python
from transformers.utils.masking_utils import validate_mask_properties

# éªŒè¯å› æœæ©ç å±æ€§
is_valid = validate_mask_properties(causal_mask, "causal")
```

### å¯è§†åŒ–å·¥å…·
```python
from transformers.utils.masking_utils import tensor_to_mask_visual

# å¯è§†åŒ–æ©ç æ¨¡å¼
visualization = tensor_to_mask_visual(causal_mask[0, 0])
print(visualization)
```

### æ€§èƒ½åˆ†æ
```python
# åˆ†æä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„æ€§èƒ½
for seq_len in [512, 1024, 2048, 4096]:
    start_time = time.time()
    mask = create_causal_mask(config, test_embeds, None, cache_pos, None)
    print(f"åºåˆ—é•¿åº¦ {seq_len}: {time.time() - start_time:.4f}s")
```

## ğŸ“– æŠ€æœ¯ç»†èŠ‚

### ç‰ˆæœ¬å…¼å®¹æ€§
- **PyTorch >= 2.6**: å®Œæ•´åŠŸèƒ½æ”¯æŒ
- **PyTorch >= 2.5**: åŸºç¡€åŠŸèƒ½æ”¯æŒ
- **PyTorch < 2.5**: é™åˆ¶åŠŸèƒ½æ”¯æŒ

### è®¾å¤‡æ”¯æŒ
- **CUDA**: å®Œå…¨æ”¯æŒï¼Œæ€§èƒ½ä¼˜åŒ–
- **CPU**: å®Œå…¨æ”¯æŒ
- **XPU**: å®Œå…¨æ”¯æŒï¼Œç‰¹æ®Šä¼˜åŒ–
- **MPS**: åŸºç¡€æ”¯æŒ

### æ•°æ®ç±»å‹
- **bool**: æ ‡å‡†æ©ç æ ¼å¼
- **float32**: Eager æ³¨æ„åŠ›æ ¼å¼
- **float16**: åŠç²¾åº¦ä¼˜åŒ–
- **bfloat16**: æ··åˆç²¾åº¦è®­ç»ƒ

## ğŸ”® é«˜çº§ç‰¹æ€§

### æ··åˆç¼“å­˜æ¶æ„
```python
# æ”¯æŒä¸åŒå±‚ä½¿ç”¨ä¸åŒç¼“å­˜ç­–ç•¥
if hasattr(past_key_values, "is_sliding"):
    # æ··åˆç¼“å­˜: éƒ¨åˆ†å±‚ä½¿ç”¨æ»‘åŠ¨çª—å£
    layer_idx = past_key_values.is_sliding.index(True)
```

### è‡ªå®šä¹‰æ©ç å‡½æ•°
```python
# å®šä¹‰è‡ªå®šä¹‰æ³¨æ„åŠ›æ¨¡å¼
def custom_mask_function(batch_idx, head_idx, q_idx, kv_idx):
    # å®ç°è‡ªå®šä¹‰é€»è¾‘
    return custom_condition(q_idx, kv_idx)

# ç»„åˆåˆ°é«˜çº§æ©ç ä¸­
custom_mask = create_causal_mask(
    config=config,
    input_embeds=input_embeds,
    attention_mask=None,
    cache_position=cache_position,
    past_key_values=None,
    or_mask_function=custom_mask_function
)
```

### åŠ¨æ€æ©ç ç”Ÿæˆ
```python
# æ ¹æ®è¾“å…¥åŠ¨æ€ç”Ÿæˆæ©ç æ¨¡å¼
def adaptive_masking(input_length, complexity_threshold):
    if input_length > complexity_threshold:
        return "sliding_attention"
    elif input_length > complexity_threshold // 2:
        return "chunked_attention"
    else:
        return "full_attention"
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

### æ·»åŠ æ–°çš„æ©ç ç±»å‹
1. åœ¨ `basic_mask_functions.py` ä¸­å®šä¹‰åŸºç¡€å‡½æ•°
2. åœ¨ `high_level_mask_creation.py` ä¸­æ·»åŠ åˆ›å»ºå‡½æ•°
3. åœ¨ `LAYER_PATTERN_TO_MASK_FUNCTION_MAPPING` ä¸­æ³¨å†Œ
4. æ·»åŠ ç›¸åº”çš„æµ‹è¯•å’Œæ–‡æ¡£

### æ€§èƒ½ä¼˜åŒ–å»ºè®®
1. ä½¿ç”¨ `torch.vmap` è¿›è¡Œå‘é‡åŒ–
2. é¿å…ä¸å¿…è¦çš„å¼ é‡åˆ›å»º
3. åˆ©ç”¨è®¾å¤‡ç‰¹å®šçš„ä¼˜åŒ–
4. è€ƒè™‘å†…å­˜è®¿é—®æ¨¡å¼

## ğŸ“š å‚è€ƒèµ„æ–™

- [PyTorch Attention æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
- [Flash Attention åŸè®ºæ–‡](https://arxiv.org/abs/2205.14135)
- [Flex Introduction](https://pytorch.org/blog/flexattention/)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers/)

## ğŸ“„ è®¸å¯è¯

æœ¬æ–‡æ¡£éµå¾ªä¸ Transformers åº“ç›¸åŒçš„ Apache 2.0 è®¸å¯è¯ã€‚

---

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªæ´»æ–‡æ¡£ï¼Œä¼šéšç€åº“çš„å‘å±•ä¸æ–­æ›´æ–°ã€‚å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– PRã€‚