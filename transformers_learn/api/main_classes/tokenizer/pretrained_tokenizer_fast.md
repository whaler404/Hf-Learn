# PreTrainedTokenizerFast æ–¹æ³•æ–‡æ¡£

`PreTrainedTokenizerFast` æ˜¯ Hugging Face Transformers åº“ä¸­å¿«é€Ÿåˆ†è¯å™¨çš„åŸºç±»ï¼Œå®ƒåŒ…è£…äº† Rust å®ç°çš„ tokenizers åº“ä»¥æä¾›é«˜æ€§èƒ½çš„åˆ†è¯åŠŸèƒ½ï¼Œç»§æ‰¿è‡ª `PreTrainedTokenizerBase`

## ç›®å½•
- [åˆå§‹åŒ–å’Œé…ç½®æ–¹æ³•](#åˆå§‹åŒ–å’Œé…ç½®æ–¹æ³•)
- [è¯æ±‡è¡¨ç›¸å…³æ–¹æ³•](#è¯æ±‡è¡¨ç›¸å…³æ–¹æ³•)
- [ç¼–ç å’Œè§£ç æ–¹æ³•](#ç¼–ç å’Œè§£ç æ–¹æ³•)
- [ç‰¹æ®Štokenå¤„ç†æ–¹æ³•](#ç‰¹æ®Štokenå¤„ç†æ–¹æ³•)
- [è®­ç»ƒå’Œä¿å­˜æ–¹æ³•](#è®­ç»ƒå’Œä¿å­˜æ–¹æ³•)
- [å†…éƒ¨å·¥å…·æ–¹æ³•](#å†…éƒ¨å·¥å…·æ–¹æ³•)

---

## åˆå§‹åŒ–å’Œé…ç½®æ–¹æ³•

### `__init__(self, *args, **kwargs)`

åˆå§‹åŒ–å¿«é€Ÿåˆ†è¯å™¨å®ä¾‹ã€‚

**ä¸»è¦å‚æ•°ï¼š**
- `tokenizer_object`: æ¥è‡ª ğŸ¤— tokenizers çš„ `Tokenizer` å¯¹è±¡
- `tokenizer_file`: æœ¬åœ° JSON æ–‡ä»¶è·¯å¾„ï¼Œè¡¨ç¤ºåºåˆ—åŒ–çš„ `Tokenizer` å¯¹è±¡
- `from_slow`: æ˜¯å¦ä»æ…¢é€Ÿåˆ†è¯å™¨è½¬æ¢ï¼Œé»˜è®¤ `False`
- `__slow_tokenizer`: æ…¢é€Ÿåˆ†è¯å™¨å®ä¾‹
- `gguf_file`: GGUF æ ¼å¼æ–‡ä»¶è·¯å¾„
- `add_prefix_space`: æ˜¯å¦æ·»åŠ å‰ç¼€ç©ºæ ¼ï¼Œé»˜è®¤ `False`

**æ”¯æŒå¤šç§åˆå§‹åŒ–æ–¹å¼ï¼š**
1. ä»ç°æœ‰çš„ tokenizer å¯¹è±¡åˆ›å»º
2. ä»åºåˆ—åŒ–çš„ tokenizer æ–‡ä»¶åŠ è½½
3. ä»æ…¢é€Ÿåˆ†è¯å™¨è½¬æ¢
4. ä» GGUF æ–‡ä»¶åˆ›å»º
5. ä½¿ç”¨é»˜è®¤çš„æ…¢é€Ÿåˆ†è¯å™¨ç±»åˆ›å»º

---

## è¯æ±‡è¡¨ç›¸å…³æ–¹æ³•

### vocab_size (å±æ€§)

```python
@property
def vocab_size(self) -> int
```

**åŠŸèƒ½ï¼š** è¿”å›åŸºç¡€è¯æ±‡è¡¨çš„å¤§å°ï¼ˆä¸åŒ…æ‹¬æ·»åŠ çš„ tokensï¼‰

**è¿”å›å€¼ï¼š**
- `int`: åŸºç¡€è¯æ±‡è¡¨å¤§å°

### get_vocab()

```python
def get_vocab(self) -> dict[str, int]
```

**åŠŸèƒ½ï¼š** è·å–åŒ…å«æ·»åŠ  tokens çš„å®Œæ•´è¯æ±‡è¡¨

**è¿”å›å€¼ï¼š**
- `dict[str, int]`: è¯æ±‡åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸

### vocab (å±æ€§)

```python
@property
def vocab(self) -> dict[str, int]
```

**åŠŸèƒ½ï¼š** è·å–è¯æ±‡è¡¨ï¼Œç­‰åŒäº `get_vocab()`

**è¿”å›å€¼ï¼š**
- `dict[str, int]`: è¯æ±‡åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸

### added_tokens_encoder (å±æ€§)

```python
@property
def added_tokens_encoder(self) -> dict[str, int]
```

**åŠŸèƒ½ï¼š** è¿”å›ä»å­—ç¬¦ä¸²åˆ°ç´¢å¼•çš„æ’åºæ˜ å°„ï¼ŒåŒ…å«æ·»åŠ çš„ tokens

**è¿”å›å€¼ï¼š**
- `dict[str, int]`: æ·»åŠ çš„ token åˆ°ç´¢å¼•çš„æ˜ å°„

### added_tokens_decoder (å±æ€§)

```python
@property
def added_tokens_decoder(self) -> dict[int, AddedToken]
```

**åŠŸèƒ½ï¼š** è¿”å›è¯æ±‡è¡¨ä¸­æ·»åŠ çš„ tokensï¼Œæ ¼å¼ä¸ºç´¢å¼•åˆ° AddedToken çš„å­—å…¸

**è¿”å›å€¼ï¼š**
- `dict[int, AddedToken]`: ç´¢å¼•åˆ° AddedToken çš„æ˜ å°„

### get_added_vocab()

```python
def get_added_vocab(self) -> dict[str, int]
```

**åŠŸèƒ½ï¼š** è¿”å›æ·»åŠ çš„ tokensï¼Œæ ¼å¼ä¸º token åˆ°ç´¢å¼•çš„å­—å…¸

**è¿”å›å€¼ï¼š**
- `dict[str, int]`: æ·»åŠ çš„ token åˆ°ç´¢å¼•çš„æ˜ å°„

---

## ç¼–ç å’Œè§£ç æ–¹æ³•

### convert_tokens_to_ids

```python
def convert_tokens_to_ids(self, tokens: Union[str, Iterable[str]]) -> Union[int, list[int]]
```

**åŠŸèƒ½ï¼š** å°† token å­—ç¬¦ä¸²ï¼ˆæˆ– token åºåˆ—ï¼‰è½¬æ¢ä¸ºæ•´æ•° idï¼ˆæˆ– id åˆ—è¡¨ï¼‰

**å‚æ•°ï¼š**
- `tokens` (`str` æˆ– `Iterable[str]`): è¦è½¬æ¢çš„ä¸€ä¸ªæˆ–å¤šä¸ª tokens

**è¿”å›å€¼ï¼š**
- `int` æˆ– `list[int]`: token id æˆ– token id åˆ—è¡¨

### convert_ids_to_tokens

```python
def convert_ids_to_tokens(
    self, ids: Union[int, list[int]], skip_special_tokens: bool = False
) -> Union[str, list[str]]
```

**åŠŸèƒ½ï¼š** å°†å•ä¸ªç´¢å¼•æˆ–ç´¢å¼•åºåˆ—è½¬æ¢ä¸º token æˆ– token åºåˆ—

**å‚æ•°ï¼š**
- `ids` (`int` æˆ– `list[int]`): è¦è½¬æ¢çš„ token id æˆ– token id åˆ—è¡¨
- `skip_special_tokens` (`bool`, å¯é€‰, é»˜è®¤ä¸º `False`): æ˜¯å¦åœ¨è§£ç æ—¶ç§»é™¤ç‰¹æ®Š tokens

**è¿”å›å€¼ï¼š**
- `str` æˆ– `list[str]`: è§£ç åçš„ token(s)

### tokenize()

```python
def tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> list[str]
```

**åŠŸèƒ½ï¼š** å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯

**å‚æ•°ï¼š**
- `text` (`str`): è¦åˆ†è¯çš„æ–‡æœ¬
- `pair` (`str`, å¯é€‰): æ–‡æœ¬å¯¹ä¸­çš„ç¬¬äºŒä¸ªæ–‡æœ¬
- `add_special_tokens` (`bool`, é»˜è®¤ä¸º `False`): æ˜¯å¦æ·»åŠ ç‰¹æ®Š tokens

**è¿”å›å€¼ï¼š**
- `list[str]`: åˆ†è¯åçš„ token åˆ—è¡¨

### convert_tokens_to_string

```python
def convert_tokens_to_string(self, tokens: list[str]) -> str
```

**åŠŸèƒ½ï¼š** å°† token åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²

**å‚æ•°ï¼š**
- `tokens` (`list[str]`): è¦è½¬æ¢çš„ token åˆ—è¡¨

**è¿”å›å€¼ï¼š**
- `str`: è§£ç åçš„å­—ç¬¦ä¸²

### _decode

```python
def _decode(
    self,
    token_ids: Union[int, list[int]],
    skip_special_tokens: bool = False,
    clean_up_tokenization_spaces: Optional[bool] = None,
    **kwargs,
) -> str
```

**åŠŸèƒ½ï¼š** å†…éƒ¨è§£ç æ–¹æ³•ï¼Œå°† token id(s) è½¬æ¢ä¸ºå­—ç¬¦ä¸²

**å‚æ•°ï¼š**
- `token_ids` (`int` æˆ– `list[int]`): è¦è§£ç çš„ token id(s)
- `skip_special_tokens` (`bool`, å¯é€‰, é»˜è®¤ä¸º `False`): æ˜¯å¦è·³è¿‡ç‰¹æ®Š tokens
- `clean_up_tokenization_spaces` (`bool`, å¯é€‰): æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼

**è¿”å›å€¼ï¼š**
- `str`: è§£ç åçš„æ–‡æœ¬

---

## ç‰¹æ®Štokenå¤„ç†æ–¹æ³•

### num_special_tokens_to_add

```python
def num_special_tokens_to_add(self, pair: bool = False) -> int
```

**åŠŸèƒ½ï¼š** è¿”å›ç¼–ç åºåˆ—æ—¶æ·»åŠ çš„ç‰¹æ®Š tokens æ•°é‡

âš ï¸ **æ³¨æ„ï¼š** æ­¤æ–¹æ³•é€šè¿‡ç¼–ç è™šæ‹Ÿè¾“å…¥å¹¶æ£€æŸ¥æ·»åŠ çš„ tokens æ•°é‡æ¥å®ç°ï¼Œå› æ­¤æ•ˆç‡ä¸é«˜ã€‚ä¸è¦åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨ã€‚

**å‚æ•°ï¼š**
- `pair` (`bool`, å¯é€‰, é»˜è®¤ä¸º `False`): æ˜¯å¦è®¡ç®—åºåˆ—å¯¹æƒ…å†µä¸‹çš„ç‰¹æ®Š tokens æ•°é‡

**è¿”å›å€¼ï¼š**
- `int`: æ·»åŠ åˆ°åºåˆ—çš„ç‰¹æ®Š tokens æ•°é‡

### set_truncation_and_padding

```python
def set_truncation_and_padding(
    self,
    padding_strategy: PaddingStrategy,
    truncation_strategy: TruncationStrategy,
    max_length: int,
    stride: int,
    pad_to_multiple_of: Optional[int],
    padding_side: Optional[str],
)
```

**åŠŸèƒ½ï¼š** ä¸ºå¿«é€Ÿåˆ†è¯å™¨å®šä¹‰æˆªæ–­å’Œå¡«å……ç­–ç•¥ï¼Œå¹¶åœ¨ä¹‹åæ¢å¤åˆ†è¯å™¨è®¾ç½®

**å‚æ•°ï¼š**
- `padding_strategy` (`PaddingStrategy`): åº”ç”¨äºè¾“å…¥çš„å¡«å……ç±»å‹
- `truncation_strategy` (`TruncationStrategy`): åº”ç”¨äºè¾“å…¥çš„æˆªæ–­ç±»å‹
- `max_length` (`int`): åºåˆ—çš„æœ€å¤§é•¿åº¦
- `stride` (`int`): å¤„ç†æº¢å‡ºæ—¶çš„æ­¥é•¿
- `pad_to_multiple_of` (`int`, å¯é€‰): å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æŒ‡å®šå€¼çš„å€æ•°
- `padding_side` (`str`, å¯é€‰): åº”ç”¨å¡«å……çš„ä¸€ä¾§ï¼Œå¯é€‰å€¼ä¸º ['right', 'left']

---

## è®­ç»ƒå’Œä¿å­˜æ–¹æ³•

### train_new_from_iterator

```python
def train_new_from_iterator(
    self,
    text_iterator,
    vocab_size,
    length=None,
    new_special_tokens=None,
    special_tokens_map=None,
    **kwargs,
)
```

**åŠŸèƒ½ï¼š** åœ¨æ–°è¯­æ–™åº“ä¸Šè®­ç»ƒåˆ†è¯å™¨ï¼Œä½¿ç”¨ä¸å½“å‰åˆ†è¯å™¨ç›¸åŒçš„é»˜è®¤è®¾ç½®ï¼ˆç‰¹æ®Š tokens æˆ–åˆ†è¯æµç¨‹ï¼‰

**å‚æ•°ï¼š**
- `text_iterator` (`generator of list[str]`): è®­ç»ƒè¯­æ–™åº“ï¼Œåº”è¯¥æ˜¯æ–‡æœ¬æ‰¹æ¬¡çš„ç”Ÿæˆå™¨
- `vocab_size` (`int`): æœŸæœ›çš„åˆ†è¯å™¨è¯æ±‡è¡¨å¤§å°
- `length` (`int`, å¯é€‰): è¿­ä»£å™¨ä¸­çš„åºåˆ—æ€»æ•°ï¼Œç”¨äºæä¾›æœ‰æ„ä¹‰çš„è¿›åº¦è·Ÿè¸ª
- `new_special_tokens` (`list of str or AddedToken`, å¯é€‰): è¦æ·»åŠ åˆ°è®­ç»ƒåˆ†è¯å™¨çš„æ–°ç‰¹æ®Š tokens åˆ—è¡¨
- `special_tokens_map` (`dict[str, str]`, å¯é€‰): å¦‚æœè¦é‡å‘½åçš„ç‰¹æ®Š tokensï¼Œæä¾›æ—§åç§°åˆ°æ–°åç§°çš„æ˜ å°„
- `**kwargs` (`dict[str, Any]`, å¯é€‰): ä¼ é€’ç»™ ğŸ¤— Tokenizers åº“è®­ç»ƒå™¨çš„é¢å¤–å…³é”®å­—å‚æ•°

**è¿”å›å€¼ï¼š**
- `PreTrainedTokenizerFast`: ä¸åŸå§‹ç±»å‹ç›¸åŒçš„æ–°åˆ†è¯å™¨ï¼Œåœ¨ `text_iterator` ä¸Šè®­ç»ƒ

### _save_pretrained

```python
def _save_pretrained(
    self,
    save_directory: Union[str, os.PathLike],
    file_names: tuple[str, ...],
    legacy_format: Optional[bool] = None,
    filename_prefix: Optional[str] = None,
) -> tuple[str, ...]
```

**åŠŸèƒ½ï¼š** ä½¿ç”¨æ…¢é€Ÿåˆ†è¯å™¨/ä¼ ç»Ÿæ ¼å¼ä¿å­˜åˆ†è¯å™¨ï¼šè¯æ±‡è¡¨ + æ·»åŠ çš„ tokensï¼Œä»¥åŠåŒ…å« {é…ç½® + è¯æ±‡è¡¨ + æ·»åŠ çš„ tokens} çš„å”¯ä¸€ JSON æ–‡ä»¶

**å‚æ•°ï¼š**
- `save_directory` (`str` æˆ– `os.PathLike`): ä¿å­˜ç›®å½•
- `file_names` (`tuple[str, ...]`): æ–‡ä»¶åå…ƒç»„
- `legacy_format` (`bool`, å¯é€‰): æ˜¯å¦ä½¿ç”¨ä¼ ç»Ÿæ ¼å¼
- `filename_prefix` (`str`, å¯é€‰): æ–‡ä»¶åå‰ç¼€

**è¿”å›å€¼ï¼š**
- `tuple[str, ...]`: ä¿å­˜çš„æ–‡ä»¶åå…ƒç»„

---

## å†…éƒ¨å·¥å…·æ–¹æ³•

### `is_fast` (å±æ€§)

```python
@property
def is_fast(self) -> bool
```

**åŠŸèƒ½ï¼š** è¿”å› Trueï¼Œæ ‡è¯†è¿™æ˜¯å¿«é€Ÿåˆ†è¯å™¨

**è¿”å›å€¼ï¼š**
- `bool`: å§‹ç»ˆä¸º True

### `can_save_slow_tokenizer` (å±æ€§)

```python
@property
def can_save_slow_tokenizer(self) -> bool
```

**åŠŸèƒ½ï¼š** è¿”å›æ˜¯å¦å¯ä»¥ä¿å­˜æ…¢é€Ÿåˆ†è¯å™¨

**è¿”å›å€¼ï¼š**
- `bool`: å¦‚æœå¯ä»¥ä¿å­˜æ…¢é€Ÿåˆ†è¯å™¨åˆ™è¿”å› True

### `__bool__()`

```python
def __bool__(self) -> bool
```

**åŠŸèƒ½ï¼š** è¿”å› Trueï¼Œé¿å…æ˜‚è´µçš„ `assert tokenizer` é™·é˜±

**è¿”å›å€¼ï¼š**
- `bool`: å§‹ç»ˆä¸º True

### `__len__()`

```python
def __len__(self) -> int
```

**åŠŸèƒ½ï¼š** è¿”å›åŒ…å«æ·»åŠ  tokens çš„å®Œæ•´è¯æ±‡è¡¨å¤§å°

**è¿”å›å€¼ï¼š**
- `int`: å®Œæ•´è¯æ±‡è¡¨å¤§å°

### `backend_tokenizer` (å±æ€§)

```python
@property
def backend_tokenizer(self) -> TokenizerFast
```

**åŠŸèƒ½ï¼š** è¿”å›ç”¨ä½œåç«¯çš„ Rust åˆ†è¯å™¨

**è¿”å›å€¼ï¼š**
- `TokenizerFast`: Rust åç«¯åˆ†è¯å™¨

### `decoder` (å±æ€§)

```python
@property
def decoder(self) -> DecoderFast
```

**åŠŸèƒ½ï¼š** è¿”å›æ­¤åˆ†è¯å™¨çš„ Rust è§£ç å™¨

**è¿”å›å€¼ï¼š**
- `DecoderFast`: Rust è§£ç å™¨

### `_convert_encoding()`

```python
def _convert_encoding(
    self,
    encoding: EncodingFast,
    return_token_type_ids: Optional[bool] = None,
    return_attention_mask: Optional[bool] = None,
    return_overflowing_tokens: bool = False,
    return_special_tokens_mask: bool = False,
    return_offsets_mapping: bool = False,
    return_length: bool = False,
    verbose: bool = True,
) -> tuple[dict[str, Any], list[EncodingFast]]
```

**åŠŸèƒ½ï¼š** å°†ç¼–ç è¡¨ç¤ºï¼ˆæ¥è‡ªåº•å±‚ HuggingFace åˆ†è¯å™¨è¾“å‡ºï¼‰è½¬æ¢ä¸º Python å­—å…¸å’Œç¼–ç åˆ—è¡¨ï¼Œå¤„ç†æ¥è‡ªæº¢å‡º tokens çš„æ‰¹æ¬¡æ„å»º

**å‚æ•°ï¼š**
- `encoding` (`EncodingFast`): è¾“å…¥ç¼–ç 
- `return_token_type_ids` (`bool`, å¯é€‰): æ˜¯å¦è¿”å› token ç±»å‹ ids
- `return_attention_mask` (`bool`, å¯é€‰): æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç 
- `return_overflowing_tokens` (`bool`, å¯é€‰): æ˜¯å¦è¿”å›æº¢å‡º tokens
- `return_special_tokens_mask` (`bool`, å¯é€‰): æ˜¯å¦è¿”å›ç‰¹æ®Š tokens æ©ç 
- `return_offsets_mapping` (`bool`, å¯é€‰): æ˜¯å¦è¿”å›åç§»æ˜ å°„
- `return_length` (`bool`, å¯é€‰): æ˜¯å¦è¿”å›é•¿åº¦
- `verbose` (`bool`, å¯é€‰): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

**è¿”å›å€¼ï¼š**
- `tuple[dict[str, Any], list[EncodingFast]]`: ç¼–ç å­—å…¸å’Œç¼–ç åˆ—è¡¨çš„å…ƒç»„

### `_convert_token_to_id_with_added_voc()`

```python
def _convert_token_to_id_with_added_voc(self, token: str) -> int
```

**åŠŸèƒ½ï¼š** å°† token è½¬æ¢ä¸º idï¼Œè€ƒè™‘æ·»åŠ çš„è¯æ±‡è¡¨

**å‚æ•°ï¼š**
- `token` (`str`): è¦è½¬æ¢çš„ token

**è¿”å›å€¼ï¼š**
- `int`: token id æˆ– unknown token id

### `_convert_id_to_token()`

```python
def _convert_id_to_token(self, index: int) -> Optional[str]
```

**åŠŸèƒ½ï¼š** å°† id è½¬æ¢ä¸º token

**å‚æ•°ï¼š**
- `index` (`int`): è¦è½¬æ¢çš„ id

**è¿”å›å€¼ï¼š**
- `Optional[str]`: å¯¹åº”çš„ tokenï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None

### `_add_tokens()`

```python
def _add_tokens(self, new_tokens: list[Union[str, AddedToken]], special_tokens=False) -> int
```

**åŠŸèƒ½ï¼š** æ·»åŠ æ–°çš„ tokens åˆ°è¯æ±‡è¡¨

**å‚æ•°ï¼š**
- `new_tokens` (`list[Union[str, AddedToken]]`): è¦æ·»åŠ çš„æ–° tokens åˆ—è¡¨
- `special_tokens` (`bool`, å¯é€‰): æ˜¯å¦ä¸ºç‰¹æ®Š tokens

**è¿”å›å€¼ï¼š**
- `int`: æ·»åŠ çš„ tokens æ•°é‡

### `_batch_encode_plus()`

```python
def _batch_encode_plus(
    self,
    batch_text_or_text_pairs: Union[
        list[TextInput], list[TextInputPair], list[PreTokenizedInput], list[PreTokenizedInputPair]
    ],
    add_special_tokens: bool = True,
    padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
    truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
    max_length: Optional[int] = None,
    stride: int = 0,
    is_split_into_words: bool = False,
    pad_to_multiple_of: Optional[int] = None,
    padding_side: Optional[str] = None,
    return_tensors: Optional[str] = None,
    return_token_type_ids: Optional[bool] = None,
    return_attention_mask: Optional[bool] = None,
    return_overflowing_tokens: bool = False,
    return_special_tokens_mask: bool = False,
    return_offsets_mapping: bool = False,
    return_length: bool = False,
    verbose: bool = True,
    split_special_tokens: bool = False,
) -> BatchEncoding
```

**åŠŸèƒ½ï¼š** æ‰¹é‡ç¼–ç æ–‡æœ¬æˆ–æ–‡æœ¬å¯¹

**å‚æ•°ï¼š**
- `batch_text_or_text_pairs`: è¦ç¼–ç çš„æ–‡æœ¬æ‰¹æ¬¡
- `add_special_tokens`: æ˜¯å¦æ·»åŠ ç‰¹æ®Š tokens
- `padding_strategy`: å¡«å……ç­–ç•¥
- `truncation_strategy`: æˆªæ–­ç­–ç•¥
- `max_length`: æœ€å¤§é•¿åº¦
- `stride`: æ­¥é•¿
- `is_split_into_words`: æ˜¯å¦å·²ç»åˆ†å‰²ä¸ºè¯è¯­
- `pad_to_multiple_of`: å¡«å……åˆ°æŒ‡å®šå€¼çš„å€æ•°
- `padding_side`: å¡«å……ä¾§
- `return_tensors`: è¿”å›å¼ é‡ç±»å‹
- å…¶ä»–å¯é€‰è¿”å›å‚æ•°...

**è¿”å›å€¼ï¼š**
- `BatchEncoding`: ç¼–ç ç»“æœ

### `_encode_plus()`

```python
def _encode_plus(
    self,
    text: Union[TextInput, PreTokenizedInput],
    text_pair: Optional[Union[TextInput, PreTokenizedInput]] = None,
    add_special_tokens: bool = True,
    padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
    truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
    max_length: Optional[int] = None,
    stride: int = 0,
    is_split_into_words: bool = False,
    pad_to_multiple_of: Optional[int] = None,
    padding_side: Optional[str] = None,
    return_tensors: Optional[bool] = None,
    return_token_type_ids: Optional[bool] = None,
    return_attention_mask: Optional[bool] = None,
    return_overflowing_tokens: bool = False,
    return_special_tokens_mask: bool = False,
    return_offsets_mapping: bool = False,
    return_length: bool = False,
    verbose: bool = True,
    split_special_tokens: bool = False,
    **kwargs,
) -> BatchEncoding
```

**åŠŸèƒ½ï¼š** ç¼–ç å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬å¯¹

**å‚æ•°ï¼š** ç±»ä¼¼ `_batch_encode_plus()` ä½†é’ˆå¯¹å•ä¸ªæ–‡æœ¬

**è¿”å›å€¼ï¼š**
- `BatchEncoding`: ç¼–ç ç»“æœ

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ä½¿ç”¨

```python
from transformers import AutoTokenizer

# åŠ è½½å¿«é€Ÿåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# ç¼–ç æ–‡æœ¬
text = "Hello, world!"
encoded = tokenizer(text)
print(encoded["input_ids"])  # [101, 7592, 1010, 2088, 999, 102]

# è§£ç æ–‡æœ¬
decoded = tokenizer.decode(encoded["input_ids"])
print(decoded)  # [CLS] hello, world! [SEP]

# è½¬æ¢ tokens å’Œ ids
tokens = tokenizer.tokenize(text)
print(tokens)  # ['hello', ',', 'world', '!']

token_ids = tokenizer.convert_tokens_to_ids(tokens)
print(token_ids)  # [7592, 1010, 2088, 999]
```

### æ‰¹é‡ç¼–ç 

```python
# æ‰¹é‡ç¼–ç 
texts = ["Hello world", "How are you?"]
encoded_batch = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
print(encoded_batch["input_ids"].shape)  # torch.Size([2, 7])
```

### è®­ç»ƒæ–°åˆ†è¯å™¨

```python
# ä½¿ç”¨ç°æœ‰åˆ†è¯å™¨è®­ç»ƒæ–°åˆ†è¯å™¨
corpus = [
    ["This is the first sentence.", "This is the second sentence."],
    ["Another example sentence.", "And one more sentence."]
]

new_tokenizer = tokenizer.train_new_from_iterator(
    corpus,
    vocab_size=30000,
    length=len(corpus)
)
```

---

## æ€§èƒ½æ³¨æ„äº‹é¡¹

1. **å¿«é€Ÿ vs æ…¢é€Ÿ**: `PreTrainedTokenizerFast` ä½¿ç”¨ Rust åç«¯ï¼Œæ¯”çº¯ Python å®ç°çš„æ…¢é€Ÿåˆ†è¯å™¨å¿«å¾—å¤š
2. **å†…å­˜æ•ˆç‡**: å¿«é€Ÿåˆ†è¯å™¨åœ¨å¤„ç†å¤§é‡æ–‡æœ¬æ—¶å†…å­˜ä½¿ç”¨æ›´é«˜æ•ˆ
3. **ç‰¹æ®Š tokens**: ä½¿ç”¨ `num_special_tokens_to_add()` æ—¶è¦æ³¨æ„æ€§èƒ½å½±å“
4. **æ‰¹å¤„ç†**: å°½é‡ä½¿ç”¨æ‰¹é‡ç¼–ç è€Œä¸æ˜¯å•ä¸ªç¼–ç ä»¥æé«˜æ€§èƒ½