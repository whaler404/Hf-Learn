# 重新排序行并拆分数据集

使用 `sort()` 根据列值的数值对其进行排序。提供的列必须与 NumPy 兼容。底层会创建一个根据列值排序的索引列表。然后，此索引映射用于访问底层 Arrow 表中的正确行。

重排操作会获取索引列表 `[0:len(my_dataset)]` 并对其进行重排，从而创建索引映射，速度下降十倍，因为需要索引映射，不能读取连续的数据块，可以切换到 `IterableDataset` 并利用其快速近似重排 `IterableDataset.shuffle() `

`select()` 根据索引列表返回行，`filter()` 返回符合指定条件的行，可以按索引进行过滤，也会在后台创建索引映射

可以将超大数据集划分为预定义数量的数据块，在 `shard()` 函数中指定 `num_shards` 参数，以确定数据集要划分的分片数量，使用 `index` 参数指定要返回的分片

# 重命名和删除列以及其他常见的列操作

`rename_column()` 重新命名数据集重点列，与原始列关联的特征实际上会移动到新的列名下，而不是直接替换原始列

删除一列或多列时，将要删除的列名提供给 `remove_columns()` 函数，相反， `select_columns()` 选择要保留的一列或多列，并删除其余列

`cast()` 函数用于转换一列或多列的特征类型，此函数接受新特征作为参数，使用 `cast_column()` 函数更改单个列的特征类型

使用 `flatten()` 函数可以将子字段提取到各自的列中

# 将处理函数应用于数据集中的每个示例

`map ()` 的主要目的是加速处理函数。将处理函数应用于数据集中的每个示例，可以单独执行，也可以批量执行，可以创建新的行和列。

使用 `map()` 删除一列，只有在将列数据传递给映射函数后，该列才会被删除，映射函数就可以在删除列之前使用它们的内容

设置了 `with_indices=True` ，也可以将 `map()` 与索引一起使用

设置 `num_proc` 参数，即可设置要使用的进程数，设置了 `with_rank=True map()` 函数也会处理进程的排名，Rank 的主要用例是跨多个 GPU 并行计算。这需要设置 `multiprocess.set_start_method("spawn")` ，

`map()` 函数支持处理批量示例。通过设置 `batched=True` 来对批量操作，

异步函数对于并行调用 API 端点很有用，例如下载图像等内容或调用模型端点

batch() 方法将数据集中的样本分组，Dataset.batch() 返回一个新的 Dataset ，其中每个 item 都是来自原始数据集的多个样本的批次

# 连接数据集

如果单独的数据集具有相同的列类型，则可以将它们连接起来。使用 concatenate_datasets() 连接数据集

通过设置 axis=1 水平连接两个数据集，只要数据集具有相同的行数

# 应用自定义格式转换

with_format() 函数会更改列的格式，使其与一些常见的数据格式兼容，手动将数据集的输出转换为张量或数组，以避免不必要的数据复制并加快数据加载速度

# 保存并导出已处理的数据集

数据集准备就绪后，将其保存为 Parquet 格式的 Hugging Face 数据集

以 Arrow 格式本地保存在磁盘上。与 Parquet 相比，Arrow 未经压缩，因此重新加载速度更快，非常适合在磁盘上本地使用和临时缓存。但由于 Arrow 数据量较大且元数据较少，因此上传/下载/查询速度比 Parquet 慢，不太适合长期存储。

使用 `save_to_disk()` 和 `load_from_disk()` 函数从磁盘重新加载数据集

