# PeftModel æ¦‚è¿°

PeftModel æ˜¯ PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰åº“çš„æ ¸å¿ƒæ¨¡å‹ç±»ï¼Œå®ƒå°è£…äº†å„ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä¸ºä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹æä¾›ç»Ÿä¸€çš„æ¥å£ã€‚PeftModel ç»§æ‰¿è‡ª `PushToHubMixin` å’Œ `torch.nn.Module`ï¼Œæ”¯æŒå¤šç§å¾®è°ƒæŠ€æœ¯ï¼Œå¦‚ LoRAã€Prefix Tuningã€Prompt Tuning ç­‰ã€‚

## æ ¸å¿ƒæ–¹æ³•

- [PeftModel.\_\_init\_\_](#__init__)
  - prompt learning: 
    - [PeftModel.add_adapter](#add_adapter)
      - [PeftModel._setup_prompt_encoder](#_setup_prompt_encoder)
        - [PrefixEncoder.\_\_init\_\_](../../adapters/prefix-tuning/PrefixEncoder.md#__init__)
  - lora
    - [BaseTuner.\_\_init\_\_](../tuners/BaseTuner.md#init)
- [PeftModel.forward](#forward)
  - [get_base_model](#get_base_model)
    - prompt learning: [PrefixEncoder.forward](../../adapters/prefix-tuning/PrefixEncoder.md#forward)
    - lora: [Linear.forward](../../adapters/lora/Linear.md#forward)

## ç±»çš„æè¿°

PeftModel æ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼ŒåŒ…å«å„ç§ PEFT æ–¹æ³•ã€‚å®ƒä¸ºé¢„è®­ç»ƒæ¨¡å‹æ·»åŠ å‚æ•°é«˜æ•ˆçš„é€‚é…å™¨ï¼Œä½¿å¾—åœ¨ä¿æŒå¤§éƒ¨åˆ†å‚æ•°å†»ç»“çš„æƒ…å†µä¸‹ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°å°±èƒ½é€‚åº”æ–°ä»»åŠ¡ã€‚è¯¥ç±»æ”¯æŒå¤šç§ PEFT æŠ€æœ¯ï¼Œå¹¶æä¾›äº†é€‚é…å™¨ç®¡ç†ã€æ¨¡å‹ä¿å­˜/åŠ è½½ã€å‚æ•°ç»Ÿè®¡ç­‰åŠŸèƒ½ã€‚

## ç±»çš„å‚æ•°

- **model** (`~transformers.PreTrainedModel`): ç”¨äº PEFT çš„åŸºç¡€ transformer æ¨¡å‹
- **peft_config** (`PeftConfig`): PEFT æ¨¡å‹çš„é…ç½®å¯¹è±¡
- **adapter_name** (`str`, *å¯é€‰*): é€‚é…å™¨çš„åç§°ï¼Œé»˜è®¤ä¸º `"default"`
- **autocast_adapter_dtype** (`bool`, *å¯é€‰*): æ˜¯å¦è‡ªåŠ¨è½¬æ¢é€‚é…å™¨æ•°æ®ç±»å‹ã€‚é»˜è®¤ä¸º `True`ã€‚ç›®å‰åªå°† float16 å’Œ bfloat16 çš„é€‚é…å™¨æƒé‡è½¬æ¢ä¸º float32ï¼Œè¿™é€šå¸¸æ˜¯ç¨³å®šè®­ç»ƒæ‰€éœ€çš„ï¼Œåªå½±å“é€‰å®šçš„ PEFT è°ƒä¼˜å™¨
- **low_cpu_mem_usage** (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`): åœ¨ meta è®¾å¤‡ä¸Šåˆ›å»ºç©ºçš„é€‚é…å™¨æƒé‡ã€‚ç”¨äºåŠ é€ŸåŠ è½½è¿‡ç¨‹

## å±æ€§

- **base_model** (`torch.nn.Module`): ç”¨äº PEFT çš„åŸºç¡€ transformer æ¨¡å‹
- **peft_config** (`PeftConfig`): PEFT æ¨¡å‹çš„é…ç½®å¯¹è±¡
- **modules_to_save** (`list` of `str`): ä¿å­˜æ¨¡å‹æ—¶è¦ä¿å­˜çš„å­æ¨¡å—åç§°åˆ—è¡¨
- **prompt_encoder** (`PromptEncoder`): å¦‚æœä½¿ç”¨ [`PromptLearningConfig`]ï¼Œåˆ™åŒ…å«ç”¨äº PEFT çš„æç¤ºç¼–ç å™¨
- **prompt_tokens** (`torch.Tensor`): å¦‚æœä½¿ç”¨ [`PromptLearningConfig`]ï¼Œåˆ™åŒ…å«ç”¨äº PEFT çš„è™šæ‹Ÿæç¤ºæ ‡è®°
- **transformer_backbone_name** (`str`): å¦‚æœä½¿ç”¨ [`PromptLearningConfig`]ï¼Œåˆ™åŒ…å«åŸºç¡€æ¨¡å‹ä¸­ transformer ä¸»å¹²çš„åç§°
- **word_embeddings** (`torch.nn.Embedding`): å¦‚æœä½¿ç”¨ [`PromptLearningConfig`]ï¼Œåˆ™åŒ…å«åŸºç¡€æ¨¡å‹ä¸­ transformer ä¸»å¹²çš„è¯åµŒå…¥

# æ–¹æ³•


## åˆå§‹åŒ–å’Œé…ç½®æ–¹æ³•

### `__init__`
- **æ–¹æ³•æè¿°**ï¼šåˆå§‹åŒ– PeftModel å®ä¾‹
- **ä¼ å…¥å‚æ•°**ï¼š
  - `model` (`PreTrainedModel`): åŸºç¡€ transformer æ¨¡å‹
  - `peft_config` (`PeftConfig`): PEFT é…ç½®å¯¹è±¡
  - `adapter_name` (`str`, é»˜è®¤ `"default"`): é€‚é…å™¨åç§°
  - `autocast_adapter_dtype` (`bool`, é»˜è®¤ `True`): æ˜¯å¦è‡ªåŠ¨è½¬æ¢é€‚é…å™¨æ•°æ®ç±»å‹
  - `low_cpu_mem_usage` (`bool`, é»˜è®¤ `False`): æ˜¯å¦ä½¿ç”¨ä½ CPU å†…å­˜
- **è¿”å›å‚æ•°**ï¼šæ— 

#### method è§£è¯»
```python
# è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–æ–¹æ³•
super().__init__()

# è®¾ç½®å½“å‰æ´»åŠ¨çš„é€‚é…å™¨åç§°
self.active_adapter = adapter_name

# ä»é…ç½®ä¸­è·å– PEFT ç±»å‹ï¼ˆå¦‚ LoRAã€Prefix Tuning ç­‰ï¼‰
self.peft_type = peft_config.peft_type

# å®šä¹‰ç‰¹æ®Šçš„å‰å‘ä¼ æ’­å‚æ•°ï¼Œè¿™äº›å‚æ•°éœ€è¦ä»ç”¨æˆ·ä¼ å…¥çš„å‚æ•°ä¸­ç§»é™¤
self.special_peft_forward_args = {"adapter_names"}

# æ£€æŸ¥æ˜¯å¦ä¸ºæç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆå¦‚ Prompt Tuningã€P-Tuning ç­‰ï¼‰
self._is_prompt_learning = peft_config.is_prompt_learning

# æ ¹æ®æ˜¯å¦ä¸ºæç¤ºå­¦ä¹ é‡‡ç”¨ä¸åŒçš„åˆå§‹åŒ–ç­–ç•¥
if self._is_prompt_learning:
    # æç¤ºå­¦ä¹ æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹ï¼Œå¹¶æ·»åŠ é€‚é…å™¨
    # åˆå§‹åŒ–é€‚é…å™¨é…ç½®å­—å…¸ï¼Œä»¥é€‚é…å™¨åç§°ä¸ºé”®
    self._peft_config = {adapter_name: peft_config}
    # ä¿å­˜å¯¹åŸºç¡€æ¨¡å‹çš„å¼•ç”¨
    self.base_model = model
    # å‘æ¨¡å‹æ·»åŠ é€‚é…å™¨ï¼ˆé…ç½®ä¸ºæç¤ºå­¦ä¹ ç±»å‹ï¼‰
    self.add_adapter(adapter_name, peft_config, low_cpu_mem_usage=low_cpu_mem_usage)
else:
    # å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚ LoRAã€AdaLoRA ç­‰ï¼‰ï¼šä½¿ç”¨ä¸“é—¨çš„è°ƒä¼˜å™¨åŒ…è£…æ¨¡å‹
    self._peft_config = None
    # æ ¹æ® PEFT ç±»å‹è·å–å¯¹åº”çš„è°ƒä¼˜å™¨ç±»ï¼ˆå¦‚ LoraModelã€AdaLoraModel ç­‰ï¼‰
    # PeftModel >> BaseTuner
    cls = PEFT_TYPE_TO_TUNER_MAPPING[peft_config.peft_type]
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ä½ CPU å†…å­˜æ¥é€‰æ‹©ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    ctx = init_empty_weights if low_cpu_mem_usage else nullcontext
    with ctx():
        # ä½¿ç”¨è°ƒä¼˜å™¨åŒ…è£…åŸºç¡€æ¨¡å‹ï¼Œä¼ å…¥é€‚é…å™¨é…ç½®æ˜ å°„å’Œé€‚é…å™¨åç§°
        self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)

# å¦‚æœåŸºç¡€æ¨¡å‹æ”¯æŒæ•°æ®ç±»å‹è½¬æ¢ï¼Œåˆ™é…ç½®é€‚é…å™¨çš„æ•°æ®ç±»å‹è‡ªåŠ¨è½¬æ¢
if hasattr(self.base_model, "_cast_adapter_dtype"):
    self.base_model._cast_adapter_dtype(
        adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype
    )

# å¦‚æœæ¨¡å‹å¯ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œåˆ™å‡†å¤‡æ¨¡å‹ä»¥æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹
if getattr(model, "is_gradient_checkpointing", True):
    model = self.prepare_model_for_gradient_checkpointing(model)

# ä¸ºäº†é¿å…æ•°å€¼å·®å¼‚å’Œæ„å¤–è¡Œä¸ºï¼Œç¦ç”¨é¢„è®­ç»ƒæ—¶çš„å¼ é‡å¹¶è¡Œæ¨¡æ‹Ÿ
# è¿™æ˜¯ä¸ºäº†è§£å†³ Pytorch çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜ï¼šhttps://github.com/pytorch/pytorch/issues/76232
if hasattr(self.base_model, "config") and hasattr(self.base_model.config, "pretraining_tp"):
    self.base_model.config.pretraining_tp = 1
```

### `add_adapter`
- **æ–¹æ³•æè¿°**ï¼šæ ¹æ®ä¼ å…¥çš„é…ç½®å‘æ¨¡å‹æ·»åŠ é€‚é…å™¨ã€‚æ­¤é€‚é…å™¨æœªç»è¿‡è®­ç»ƒã€‚è¦åŠ è½½è®­ç»ƒå¥½çš„é€‚é…å™¨ï¼Œè¯·ä½¿ç”¨ [`PeftModel.load_adapter`]ã€‚æ–°é€‚é…å™¨çš„åç§°åº”è¯¥æ˜¯å”¯ä¸€çš„ã€‚æ–°é€‚é…å™¨ä¸ä¼šè‡ªåŠ¨è®¾ç½®ä¸ºæ´»åŠ¨é€‚é…å™¨ã€‚
- **ä¼ å…¥å‚æ•°**ï¼š
  - `adapter_name` (`str`): è¦æ·»åŠ çš„é€‚é…å™¨åç§°
  - `peft_config` (`PeftConfig`): è¦æ·»åŠ çš„é€‚é…å™¨é…ç½®
  - `low_cpu_mem_usage` (`bool`, *å¯é€‰*, é»˜è®¤ `False`): åœ¨ meta è®¾å¤‡ä¸Šåˆ›å»ºç©ºçš„é€‚é…å™¨æƒé‡ã€‚ç”¨äºåŠ é€ŸåŠ è½½ä¿å­˜é€‚é…å™¨çš„è¿‡ç¨‹ã€‚åˆ›å»ºæ–°çš„ PEFT é€‚é…å™¨è¿›è¡Œè®­ç»ƒæ—¶ä¸è¦ä½¿ç”¨æ­¤é€‰é¡¹
- **è¿”å›å‚æ•°**ï¼šæ— 

#### method è§£è¯»
```python
# æ ¹æ®é€‚é…å™¨ç±»å‹è·å–å¯¹åº”çš„å‰ç¼€æ˜ å°„ï¼Œç”¨äºæ£€æŸ¥é€‚é…å™¨åç§°æ˜¯å¦åˆè§„
prefix = PEFT_TYPE_TO_PREFIX_MAPPING.get(peft_config.peft_type)

# æ£€æŸ¥é€‚é…å™¨åç§°æ˜¯å¦åŒ…å«åœ¨ç±»å‹å‰ç¼€ä¸­ï¼Œå¦‚æœåŒ…å«åˆ™å‘å‡ºè­¦å‘Š
# è¿™å¯èƒ½ä¼šå¯¼è‡´åŠ è½½æ—¶é€‚é…å™¨æƒé‡çš„é‡æ–°åˆå§‹åŒ–
if prefix and adapter_name in prefix:
    warnings.warn(
        f"Adapter name {adapter_name} should not be contained in the prefix {prefix}."
        "This may lead to reinitialization of the adapter weights during loading."
    )

# æ£€æŸ¥æ–°é€‚é…å™¨çš„ç±»å‹æ˜¯å¦ä¸å½“å‰æ¨¡å‹çš„ PEFT ç±»å‹ä¸€è‡´
# ä¸å…è®¸åœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸­æ··åˆä¸åŒç±»å‹çš„é€‚é…å™¨ï¼ˆå¦‚ LoRA å’Œ Prefix Tuningï¼‰
if peft_config.peft_type != self.peft_type:
    raise ValueError(
        f"Cannot combine adapters with different peft types. "
        f"Found {self.peft_type} and {peft_config.peft_type}."
    )

try:
    # æ ¹æ®é€‚é…å™¨ç±»å‹é‡‡ç”¨ä¸åŒçš„æ·»åŠ ç­–ç•¥
    if peft_config.is_prompt_learning:
        # æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆå¦‚ Prompt Tuningã€P-Tuning ç­‰ï¼‰
        # å°†é€‚é…å™¨é…ç½®æ·»åŠ åˆ°é…ç½®å­—å…¸ä¸­
        self.peft_config[adapter_name] = peft_config

        # è·å–æ¨¡å‹é…ç½®çš„å­—å…¸è¡¨ç¤º
        if hasattr(self.config, "to_dict"):
            dict_config = self.config.to_dict()
        else:
            dict_config = self.config

        # å‡†å¤‡æç¤ºå­¦ä¹ é…ç½®ï¼Œç¡®ä¿ä¸æ¨¡å‹é…ç½®å…¼å®¹
        peft_config = _prepare_prompt_learning_config(peft_config, dict_config)

        # è®¾ç½®æç¤ºç¼–ç å™¨ï¼Œå¤„ç†æç¤ºçš„è¡¨ç¤ºå’Œå­¦ä¹ 
        self._setup_prompt_encoder(adapter_name)

        # è®¾ç½®é¢å¤–çš„å¯è®­ç»ƒæ¨¡å—ï¼ˆå¦‚ç‰¹å®šçš„å±‚æˆ–å‚æ•°ï¼‰
        set_additional_trainable_modules(
            model=self.base_model,
            peft_config=peft_config,
            model_config=BaseTuner.get_model_config(self),
            adapter_name=adapter_name,
        )
    elif peft_config.is_adaption_prompt:
        # é€‚é…æç¤ºæ–¹æ³•ï¼ˆAdaption Promptï¼‰
        # é€šè¿‡åŸºç¡€æ¨¡å‹æ·»åŠ é€‚é…å™¨
        self.base_model.add_adapter(adapter_name, peft_config)

        # è®¾ç½®é¢å¤–çš„å¯è®­ç»ƒæ¨¡å—
        set_additional_trainable_modules(
            model=self.base_model,
            peft_config=peft_config,
            model_config=BaseTuner.get_model_config(self),
            adapter_name=adapter_name,
        )
    else:
        # å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚ LoRAã€AdaLoRA ç­‰ï¼‰
        # å°†é€‚é…å™¨é…ç½®æ·»åŠ åˆ°é…ç½®å­—å…¸ä¸­
        self.peft_config[adapter_name] = peft_config

        # å‘åŸºç¡€æ¨¡å‹æ³¨å…¥é€‚é…å™¨ï¼Œè¿™ä¼šåœ¨ç›®æ ‡æ¨¡å—ä¸­æ·»åŠ é€‚é…å™¨å±‚
        self.base_model.inject_adapter(
            self.base_model.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage
        )
except Exception:  # å¦‚æœæ·»åŠ è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œæ‰§è¡Œå›æ»šæ“ä½œ
    # ä»é…ç½®å­—å…¸ä¸­ç§»é™¤å·²æ·»åŠ çš„é€‚é…å™¨é…ç½®ï¼Œä¿æŒæ¨¡å‹çŠ¶æ€ä¸€è‡´æ€§
    if adapter_name in self.peft_config:
        del self.peft_config[adapter_name]
    # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“æ·»åŠ å¤±è´¥
    raise
```

### `delete_adapter`
- **æ–¹æ³•æè¿°**ï¼šåˆ é™¤ç°æœ‰é€‚é…å™¨
- **ä¼ å…¥å‚æ•°**ï¼š
  - `adapter_name` (`str`): è¦åˆ é™¤çš„é€‚é…å™¨åç§°
- **è¿”å›å‚æ•°**ï¼šæ— 

### `set_adapter`
- **æ–¹æ³•æè¿°**ï¼šè®¾ç½®æ´»åŠ¨é€‚é…å™¨ã€‚ä¸€æ¬¡åªèƒ½æœ‰ä¸€ä¸ªé€‚é…å™¨å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚æ­¤å¤–ï¼Œæ­¤å‡½æ•°å°†æŒ‡å®šçš„é€‚é…å™¨è®¾ç½®ä¸ºå¯è®­ç»ƒï¼ˆå³ requires_grad=Trueï¼‰ã€‚å¦‚æœä¸éœ€è¦è¿™æ ·ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç 
- **ä¼ å…¥å‚æ•°**ï¼š
  - `adapter_name` (`str`): è¦è®¾ç½®ä¸ºæ´»åŠ¨é€‚é…å™¨çš„é€‚é…å™¨åç§°ã€‚é€‚é…å™¨å¿…é¡»å…ˆåŠ è½½
- **è¿”å›å‚æ•°**ï¼šæ— 

## ä¿å­˜å’ŒåŠ è½½æ–¹æ³•

### `save_pretrained`
- **æ–¹æ³•æè¿°**ï¼šå°†é€‚é…å™¨æ¨¡å‹å’Œé€‚é…å™¨é…ç½®æ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨ [`PeftModel.from_pretrained`] ç±»æ–¹æ³•é‡æ–°åŠ è½½ï¼Œä¹Ÿå¯ä»¥è¢« [`PeftModel.push_to_hub`] æ–¹æ³•ä½¿ç”¨
- **ä¼ å…¥å‚æ•°**ï¼š
  - `save_directory` (`str`): ä¿å­˜é€‚é…å™¨æ¨¡å‹å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨å°†åˆ›å»ºï¼‰
  - `safe_serialization` (`bool`, *å¯é€‰*): æ˜¯å¦ä»¥ safetensors æ ¼å¼ä¿å­˜é€‚é…å™¨æ–‡ä»¶ï¼Œé»˜è®¤ä¸º `True`
  - `selected_adapters` (`List[str]`, *å¯é€‰*): è¦ä¿å­˜çš„é€‚é…å™¨åˆ—è¡¨ã€‚å¦‚æœä¸º `None`ï¼Œå°†é»˜è®¤ä¿å­˜æ‰€æœ‰é€‚é…å™¨
  - `save_embedding_layers` (`Union[bool, str]`, *å¯é€‰*, é»˜è®¤ `"auto"`): å¦‚æœä¸º `True`ï¼Œé™¤äº†é€‚é…å™¨æƒé‡å¤–è¿˜ä¿å­˜åµŒå…¥å±‚ã€‚å¦‚æœä¸º `"auto"`ï¼Œåœ¨é…ç½®çš„ `target_modules` ä¸­æ£€æŸ¥å¸¸è§åµŒå…¥å±‚ `peft.utils.other.EMBEDDING_LAYER_NAMES`ï¼Œå¹¶è‡ªåŠ¨è®¾ç½®å¸ƒå°”æ ‡å¿—ã€‚è¿™åªé€‚ç”¨äº ğŸ¤— transformers æ¨¡å‹
  - `is_main_process` (`bool`, *å¯é€‰*): è°ƒç”¨æ­¤æ–¹æ³•çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚é»˜è®¤ä¸º `True`ã€‚å¦‚æœä¸æ˜¯ä¸»è¿›ç¨‹åˆ™ä¸ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œè¿™å¯¹äºå¤šè®¾å¤‡è®¾ç½®ï¼ˆå¦‚ DDPï¼‰å¾ˆé‡è¦
  - `path_initial_model_for_weight_conversion` (`str`, *å¯é€‰*): åˆå§‹åŒ–é€‚é…å™¨çš„è·¯å¾„ï¼Œåœ¨ç”¨ PiSSA/CorDA/OLoRA åˆå§‹åŒ–æ¨¡å‹åä½†åœ¨è¿›è¡Œä»»ä½•è®­ç»ƒä¹‹å‰è·å¾—ã€‚å½“ `path_initial_model_for_weight_conversion` ä¸ä¸º `None` æ—¶ï¼Œè®¡ç®—å¾®è°ƒå‰åé€‚é…å™¨çš„å·®å¼‚ã€‚è¿™ç§å·®å¼‚å¯ä»¥è¡¨ç¤ºä¸ºæ ‡å‡† LoRA é€‚é…å™¨çš„å‚æ•°ã€‚ä½¿ç”¨æ­¤è½¬æ¢çš„é€‚é…å™¨ä¸éœ€è¦æ›´æ”¹åŸºç¡€æ¨¡å‹ï¼Œä»è€Œæ–¹ä¾¿åœ°å…è®¸å¤šä¸ª PiSSA/CorDA/OLoRA é€‚é…å™¨ä¸ LoRA é€‚é…å™¨ä¸€èµ·ä½¿ç”¨ï¼Œä»¥åŠä»»ä½•é€‚é…å™¨çš„æ¿€æ´»æˆ–åœç”¨
  - `kwargs` (é¢å¤–çš„å…³é”®å­—å‚æ•°, *å¯é€‰*): ä¼ é€’ç»™ `push_to_hub` æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°
- **è¿”å›å‚æ•°**ï¼šæ— 

### `from_pretrained`
- **æ–¹æ³•æè¿°**ï¼šä»é¢„è®­ç»ƒæ¨¡å‹å’ŒåŠ è½½çš„ PEFT æƒé‡å®ä¾‹åŒ– PEFT æ¨¡å‹ã€‚æ³¨æ„ä¼ å…¥çš„ `model` å¯èƒ½ä¼šè¢«å°±åœ°ä¿®æ”¹
- **ä¼ å…¥å‚æ•°**ï¼š
  - `model` (`torch.nn.Module`): è¦é€‚é…çš„æ¨¡å‹ã€‚å¯¹äº ğŸ¤— Transformers æ¨¡å‹ï¼Œæ¨¡å‹åº”ä½¿ç”¨ [`~transformers.PreTrainedModel.from_pretrained`] åˆå§‹åŒ–
  - `model_id` (`str` æˆ– `os.PathLike`): è¦ä½¿ç”¨çš„ PEFT é…ç½®çš„åç§°ã€‚å¯ä»¥æ˜¯ï¼š
    - å­—ç¬¦ä¸²ï¼Œæ‰˜ç®¡åœ¨ Hugging Face Hub æ¨¡å‹ä»“åº“å†…çš„ PEFT é…ç½®çš„ `model id`
    - åŒ…å«ä½¿ç”¨ `save_pretrained` æ–¹æ³•ä¿å­˜çš„ PEFT é…ç½®æ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼ˆ`./my_peft_config_directory/`ï¼‰
  - `adapter_name` (`str`, *å¯é€‰*, é»˜è®¤ `"default"`): è¦åŠ è½½çš„é€‚é…å™¨åç§°ã€‚è¿™å¯¹äºåŠ è½½å¤šä¸ªé€‚é…å™¨å¾ˆæœ‰ç”¨
  - `is_trainable` (`bool`, *å¯é€‰*, é»˜è®¤ `False`): é€‚é…å™¨æ˜¯å¦åº”è¯¥æ˜¯å¯è®­ç»ƒçš„ã€‚å¦‚æœä¸º `False`ï¼Œé€‚é…å™¨å°†è¢«å†»ç»“ï¼Œåªèƒ½ç”¨äºæ¨ç†
  - `config` (`~peft.PeftConfig`, *å¯é€‰*): è¦ä½¿ç”¨çš„é…ç½®å¯¹è±¡ï¼Œè€Œä¸æ˜¯è‡ªåŠ¨åŠ è½½çš„é…ç½®ã€‚æ­¤é…ç½®å¯¹è±¡ä¸ `model_id` å’Œ `kwargs` äº’æ–¥ã€‚å½“é…ç½®åœ¨è°ƒç”¨ `from_pretrained` ä¹‹å‰å·²ç»åŠ è½½æ—¶å¾ˆæœ‰ç”¨
  - `autocast_adapter_dtype` (`bool`, *å¯é€‰*): æ˜¯å¦è‡ªåŠ¨è½¬æ¢é€‚é…å™¨æ•°æ®ç±»å‹ã€‚é»˜è®¤ä¸º `True`ã€‚ä»…é€‚ç”¨äºç‰¹å®šé€‚é…å™¨ç±»å‹
  - `ephemeral_gpu_offload` (`bool`, *å¯é€‰*): æ˜¯å¦å¯¹éƒ¨åˆ†åŠ è½½çš„æ¨¡å—ä½¿ç”¨ä¸´æ—¶ GPU å¸è½½ã€‚é»˜è®¤ä¸º `False`ã€‚å½“æ¨¡å‹å’Œ/æˆ–ç»„ä»¶ï¼ˆå¦‚é€‚é…å™¨ï¼‰çš„éƒ¨åˆ†åœ¨éœ€è¦ä¹‹å‰ä¿æŒåœ¨ CPU å†…å­˜ä¸­æ—¶å¾ˆæœ‰ç”¨
  - `low_cpu_mem_usage` (`bool`, *å¯é€‰*, é»˜è®¤ `False`): åœ¨åŠ è½½ä¿å­˜çš„æƒé‡ä¹‹å‰åœ¨ meta è®¾å¤‡ä¸Šåˆ›å»ºç©ºçš„é€‚é…å™¨æƒé‡ã€‚ç”¨äºåŠ é€Ÿè¿‡ç¨‹
  - `torch_device` (`str`, *å¯é€‰*, é»˜è®¤ None): åŠ è½½é€‚é…å™¨çš„è®¾å¤‡ã€‚å¦‚æœä¸º `None`ï¼Œå°†æ¨æ–­è®¾å¤‡
  - `key_mapping` (`dict`, *å¯é€‰*, é»˜è®¤ None): åœ¨åŠ è½½ `state_dict` ä¹‹å‰åº”ç”¨çš„ PEFT `state_dict` é”®çš„é¢å¤–æ˜ å°„ã€‚åº”ç”¨æ­¤æ˜ å°„æ—¶ï¼Œä¼šæå‰ç§»é™¤ PEFT ç‰¹å®šçš„ `"base_model.model"` å‰ç¼€ï¼Œå¹¶ä¸”å°šæœªæ’å…¥é€‚é…å™¨åç§°ï¼ˆä¾‹å¦‚ `"default"`ï¼‰ã€‚åªæœ‰åœ¨ä½ äº†è§£è‡ªå·±åœ¨åšä»€ä¹ˆæ—¶æ‰ä¼ é€’æ­¤å‚æ•°
  - `kwargs` (`å¯é€‰`): ä¼ é€’ç»™ç‰¹å®š PEFT é…ç½®ç±»çš„é¢å¤–å…³é”®å­—å‚æ•°
- **è¿”å›å‚æ•°**ï¼š`PeftModel` å®ä¾‹

### `load_adapter`
- **æ–¹æ³•æè¿°**ï¼šå°†è®­ç»ƒå¥½çš„é€‚é…å™¨åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚æ–°é€‚é…å™¨çš„åç§°åº”è¯¥æ˜¯å”¯ä¸€çš„ã€‚æ–°é€‚é…å™¨ä¸ä¼šè‡ªåŠ¨è®¾ç½®ä¸ºæ´»åŠ¨é€‚é…å™¨ã€‚ä½¿ç”¨ [`PeftModel.set_adapter`] è®¾ç½®æ´»åŠ¨é€‚é…å™¨
- **ä¼ å…¥å‚æ•°**ï¼š
  - `model_id` (`str` æˆ– `os.PathLike`): è¦ä½¿ç”¨çš„ PEFT é…ç½®çš„åç§°ã€‚å¯ä»¥æ˜¯ï¼š
    - å­—ç¬¦ä¸²ï¼Œæ‰˜ç®¡åœ¨ Hugging Face Hub æ¨¡å‹ä»“åº“å†…çš„ PEFT é…ç½®çš„ `model id`
    - åŒ…å«ä½¿ç”¨ `save_pretrained` æ–¹æ³•ä¿å­˜çš„ PEFT é…ç½®æ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼ˆ`./my_peft_config_directory/`ï¼‰
  - `adapter_name` (`str`): è¦æ·»åŠ çš„é€‚é…å™¨åç§°
  - `is_trainable` (`bool`, *å¯é€‰*, é»˜è®¤ `False`): é€‚é…å™¨æ˜¯å¦åº”è¯¥æ˜¯å¯è®­ç»ƒçš„ã€‚å¦‚æœä¸º `False`ï¼Œé€‚é…å™¨å°†è¢«å†»ç»“ï¼Œåªèƒ½ç”¨äºæ¨ç†
  - `torch_device` (`str`, *å¯é€‰*, é»˜è®¤ None): åŠ è½½é€‚é…å™¨çš„è®¾å¤‡ã€‚å¦‚æœä¸º `None`ï¼Œå°†æ¨æ–­è®¾å¤‡
  - `autocast_adapter_dtype` (`bool`, *å¯é€‰*, é»˜è®¤ `True`): æ˜¯å¦è‡ªåŠ¨è½¬æ¢é€‚é…å™¨æ•°æ®ç±»å‹ã€‚é»˜è®¤ä¸º `True`ã€‚ç°åœ¨è¿™åªå°†ä½¿ç”¨ float16 å’Œ bfloat16 çš„é€‚é…å™¨æƒé‡è½¬æ¢ä¸º float32ï¼Œè¿™é€šå¸¸æ˜¯ç¨³å®šè®­ç»ƒæ‰€éœ€çš„ï¼Œåªå½±å“é€‰å®šçš„ PEFT è°ƒä¼˜å™¨
  - `ephemeral_gpu_offload` (`bool`, *å¯é€‰*, é»˜è®¤ `False`): æ˜¯å¦å¯¹éƒ¨åˆ†åŠ è½½çš„æ¨¡å—ä½¿ç”¨ä¸´æ—¶ GPU å¸è½½ã€‚é»˜è®¤ä¸º `False`
  - `low_cpu_mem_usage` (`bool`, *å¯é€‰*, é»˜è®¤ `False`): åœ¨åŠ è½½ä¿å­˜çš„æƒé‡ä¹‹å‰åœ¨ meta è®¾å¤‡ä¸Šåˆ›å»ºç©ºçš„é€‚é…å™¨æƒé‡ã€‚ç”¨äºåŠ é€Ÿè¿‡ç¨‹
  - `key_mapping` (`dict`, *å¯é€‰*, é»˜è®¤ None): åœ¨åŠ è½½ `state_dict` ä¹‹å‰åº”ç”¨çš„ PEFT `state_dict` é”®çš„é¢å¤–æ˜ å°„
  - `kwargs` (`å¯é€‰`): ä¿®æ”¹é€‚é…å™¨åŠ è½½æ–¹å¼çš„é¢å¤–å‚æ•°ï¼Œä¾‹å¦‚ Hugging Face Hub çš„ä»¤ç‰Œ
- **è¿”å›å‚æ•°**ï¼šåŠ è½½ç»“æœ

## å‰å‘ä¼ æ’­å’Œç”Ÿæˆæ–¹æ³•

### `forward`
- **æ–¹æ³•æè¿°**ï¼šæ¨¡å‹çš„å‰å‘ä¼ æ’­
- **ä¼ å…¥å‚æ•°**ï¼š
  - `*args`: ä½ç½®å‚æ•°
  - `**kwargs`: å…³é”®å­—å‚æ•°
- **è¿”å›å‚æ•°**ï¼šæ¨¡å‹çš„è¾“å‡º

### `generate`
- **æ–¹æ³•æè¿°**ï¼šç”Ÿæˆåºåˆ—
- **ä¼ å…¥å‚æ•°**ï¼š
  - `*args`: ä½ç½®å‚æ•°
  - `**kwargs`: å…³é”®å­—å‚æ•°
- **è¿”å›å‚æ•°**ï¼šç”Ÿæˆçš„åºåˆ—

### `prepare_inputs_for_generation`
- **æ–¹æ³•æè¿°**ï¼šä¸ºç”Ÿæˆå‡†å¤‡è¾“å…¥ï¼ˆä»…é€‚ç”¨äºç‰¹å®šæ¨¡å‹ç±»ï¼‰
- **ä¼ å…¥å‚æ•°**ï¼š
  - `*args`: ä½ç½®å‚æ•°
  - `task_ids` (`torch.Tensor`, *å¯é€‰*): ä»»åŠ¡ ID
  - `**kwargs`: å…³é”®å­—å‚æ•°
- **è¿”å›å‚æ•°**ï¼šå‡†å¤‡å¥½çš„æ¨¡å‹è¾“å…¥

## å‚æ•°ç»Ÿè®¡å’ŒçŠ¶æ€æŸ¥è¯¢æ–¹æ³•

### `get_nb_trainable_parameters`
- **æ–¹æ³•æè¿°**ï¼šè¿”å›æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å’Œæ‰€æœ‰å‚æ•°çš„æ•°é‡
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼š`tuple[int, int]` - (å¯è®­ç»ƒå‚æ•°æ•°é‡, æ‰€æœ‰å‚æ•°æ•°é‡)

### `print_trainable_parameters`
- **æ–¹æ³•æè¿°**ï¼šæ‰“å°æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æ³¨æ„ï¼šprint_trainable_parameters() ä½¿ç”¨ get_nb_trainable_parameters()ï¼Œè¿™ä¸æ¥è‡ª huggingface/transformers çš„ num_parameters(only_trainable=True) ä¸åŒã€‚get_nb_trainable_parameters() è¿”å›åŒ…å«ä¿®æ”¹çš„ä¸»å¹² transformer æ¨¡å‹çš„ Peft æ¨¡å‹çš„ï¼ˆå¯è®­ç»ƒå‚æ•°ï¼Œæ‰€æœ‰å‚æ•°ï¼‰ã€‚å¯¹äºåƒ LoRA è¿™æ ·çš„æŠ€æœ¯ï¼Œä¸»å¹² transformer æ¨¡å‹è¢«å°±åœ°ä¿®æ”¹ã€‚ç„¶è€Œï¼Œå¯¹äºæç¤ºè°ƒä¼˜ï¼Œä¸»å¹² transformer æ¨¡å‹æœªè¢«ä¿®æ”¹ã€‚num_parameters(only_trainable=True) è¿”å›ä¸»å¹² transformer æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°æ•°é‡ï¼Œè¿™å¯èƒ½ä¸åŒ
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼šæ— 

### `get_layer_status`
- **æ–¹æ³•æè¿°**ï¼šè·å–æ¨¡å‹ä¸­æ¯ä¸ªé€‚é…å™¨å±‚çš„çŠ¶æ€ã€‚æ­¤æ–¹æ³•è¿”å› `TunerLayerStatus` æ•°æ®ç±»å®ä¾‹çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä¾‹åŒ…å«ä»¥ä¸‹å±æ€§ï¼š
  - `name` (`str`): é€‚é…å™¨å±‚çš„åç§°ï¼Œä¾‹å¦‚ `model.encoder.block.0.layer.0.SelfAttention.q`
  - `module_type` (`str`): é€‚é…å™¨å±‚çš„ç±»å‹ï¼Œä¾‹å¦‚ `lora.Linear`
  - `enabled` (`bool`): é€‚é…å™¨å±‚æ˜¯å¦å¯ç”¨
  - `active_adapters` (`list[str]`): æ´»åŠ¨é€‚é…å™¨çš„åç§°ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œä¾‹å¦‚ `["default"]`
  - `merged_adapters` (`list[str]`): åˆå¹¶é€‚é…å™¨çš„åç§°ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œä¾‹å¦‚ `["default"]`
  - `available_adapters` (`list[str]`): å¯ç”¨é€‚é…å™¨çš„åç§°ï¼Œä¾‹å¦‚ `["default"]`
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼š`list[TunerLayerStatus]` - åŒ…å«ç›¸åº”é€‚é…å™¨å±‚çŠ¶æ€çš„æ•°æ®ç±»åˆ—è¡¨

### `get_model_status`
- **æ–¹æ³•æè¿°**ï¼šè·å–æ¨¡å‹è°ƒä¼˜å™¨çš„çŠ¶æ€ã€‚æ­¤æ–¹æ³•è¿”å› `TunerModelStatus` æ•°æ®ç±»å®ä¾‹ï¼ŒåŒ…å«ä»¥ä¸‹å±æ€§ï¼š
  - `base_model_type` (`str`): åŸºç¡€æ¨¡å‹çš„ç±»å‹ï¼Œä¾‹å¦‚ `T5Model`
  - `adapter_model_type` (`str`): é€‚é…å™¨æ¨¡å‹çš„ç±»å‹ï¼Œä¾‹å¦‚ `LoraModel`
  - `peft_types` (`dict[str, str]`): é€‚é…å™¨åç§°åˆ°é€‚é…å™¨ç±»å‹çš„æ˜ å°„ï¼Œä¾‹å¦‚ `{"default": "LORA"}`
  - `trainable_params` (`int`): æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
  - `total_params` (`int`): æ¨¡å‹ä¸­å‚æ•°çš„æ€»æ•°
  - `num_adapter_layers` (`int`): æ¨¡å‹ä¸­é€‚é…å™¨å±‚çš„æ•°é‡
  - `enabled` (`bool`, `Literal["irregular"]`): æ˜¯å¦æ‰€æœ‰é€‚é…å™¨å±‚éƒ½å¯ç”¨ã€‚å¦‚æœæœ‰äº›å¯ç”¨æœ‰äº›ä¸å¯ç”¨ï¼Œè¿™å°†æ˜¯ `"irregular"`ã€‚è¿™æ„å‘³ç€æ‚¨çš„æ¨¡å‹å¤„äºä¸ä¸€è‡´çŠ¶æ€ï¼Œå¯èƒ½æ— æ³•æŒ‰é¢„æœŸå·¥ä½œ
  - `active_adapters` (`list[str]`, `Literal["irregular"]`): æ´»åŠ¨é€‚é…å™¨çš„åç§°ã€‚å¦‚æœæ´»åŠ¨é€‚é…å™¨åœ¨æ‰€æœ‰å±‚ä¸­ä¸ä¸€è‡´ï¼Œè¿™å°†æ˜¯ `"irregular"`ï¼Œè¿™æ„å‘³ç€æ‚¨çš„æ¨¡å‹å¤„äºä¸ä¸€è‡´çŠ¶æ€ï¼Œå¯èƒ½æ— æ³•æŒ‰é¢„æœŸå·¥ä½œ
  - `merged_adapters` (`list[str]`, `Literal["irregular"]`): åˆå¹¶é€‚é…å™¨çš„åç§°ã€‚å¦‚æœåˆå¹¶é€‚é…å™¨åœ¨æ‰€æœ‰å±‚ä¸­ä¸ä¸€è‡´ï¼Œè¿™å°†æ˜¯ `"irregular"`ï¼Œè¿™æ„å‘³ç€æ‚¨çš„æ¨¡å‹å¤„äºä¸ä¸€è‡´çŠ¶æ€ï¼Œå¯èƒ½æ— æ³•æŒ‰é¢„æœŸå·¥ä½œ
  - `available_adapters` (`list[str]`): å¯ç”¨é€‚é…å™¨çš„åç§°ï¼Œä¾‹å¦‚ `["default"]`
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼š`TunerModelStatus` - åŒ…å«æ¨¡å‹çŠ¶æ€çš„æ•°æ®ç±»

## å±æ€§è®¿é—®å™¨æ–¹æ³•

### `peft_config`
- **æ–¹æ³•æè¿°**ï¼šè·å– PEFT é…ç½®çš„å±æ€§è®¿é—®å™¨
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼š`dict[str, PeftConfig]` - é€‚é…å™¨åç§°åˆ°é…ç½®å¯¹è±¡çš„æ˜ å°„

### `active_adapters`
- **æ–¹æ³•æè¿°**ï¼šè·å–æ´»åŠ¨é€‚é…å™¨åˆ—è¡¨çš„å±æ€§è®¿é—®å™¨
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼š`list[str]` - æ´»åŠ¨é€‚é…å™¨åç§°åˆ—è¡¨

### `base_model_torch_dtype`
- **æ–¹æ³•æè¿°**ï¼šè·å–åŸºç¡€æ¨¡å‹ torch æ•°æ®ç±»å‹çš„å±æ€§è®¿é—®å™¨
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼šåŸºç¡€æ¨¡å‹çš„æ•°æ®ç±»å‹æˆ– None

### `active_peft_config`
- **æ–¹æ³•æè¿°**ï¼šè·å–æ´»åŠ¨ PEFT é…ç½®çš„å±æ€§è®¿é—®å™¨
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼šå½“å‰æ´»åŠ¨é€‚é…å™¨çš„ PEFT é…ç½®

### `modules_to_save`
- **æ–¹æ³•æè¿°**ï¼šè·å–è¦ä¿å­˜æ¨¡å—çš„å±æ€§è®¿é—®å™¨
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼š`Optional[set[str]]` - è¦ä¿å­˜çš„æ¨¡å—åç§°é›†åˆï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸º None

## å·¥å…·å’Œè¾…åŠ©æ–¹æ³•

### `get_base_model`
- **æ–¹æ³•æè¿°**ï¼šè¿”å›åŸºç¡€æ¨¡å‹
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼š`torch.nn.Module` - åŸºç¡€æ¨¡å‹å®ä¾‹

### `disable_adapter`
- **æ–¹æ³•æè¿°**ï¼šç¦ç”¨é€‚é…å™¨æ¨¡å—çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚ä½¿ç”¨å®ƒåœ¨åŸºç¡€æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†
- **ä¼ å…¥å‚æ•°**ï¼šæ— 
- **è¿”å›å‚æ•°**ï¼šä¸Šä¸‹æ–‡ç®¡ç†å™¨

### `prepare_model_for_gradient_checkpointing`
- **æ–¹æ³•æè¿°**ï¼šåœ¨å¿…è¦æ—¶ä¸ºæ¢¯åº¦æ£€æŸ¥ç‚¹å‡†å¤‡æ¨¡å‹
- **ä¼ å…¥å‚æ•°**ï¼š
  - `model` (`PreTrainedModel`): è¦å‡†å¤‡çš„æ¨¡å‹
- **è¿”å›å‚æ•°**ï¼šå‡†å¤‡å¥½çš„æ¨¡å‹

### `create_or_update_model_card`
- **æ–¹æ³•æè¿°**ï¼šæ›´æ–°æˆ–åˆ›å»ºæ¨¡å‹å¡ç‰‡ä»¥åŒ…å«å…³äº peft çš„ä¿¡æ¯ï¼š
  1. æ·»åŠ  `peft` åº“æ ‡ç­¾
  2. æ·»åŠ  peft ç‰ˆæœ¬
  3. æ·»åŠ åŸºç¡€æ¨¡å‹ä¿¡æ¯
  4. å¦‚æœä½¿ç”¨ï¼Œæ·»åŠ é‡åŒ–ä¿¡æ¯
- **ä¼ å…¥å‚æ•°**ï¼š
  - `output_dir` (`str`): è¾“å‡ºç›®å½•
- **è¿”å›å‚æ•°**ï¼šæ— 

## æç¤ºå­¦ä¹ ç›¸å…³æ–¹æ³•

### `_setup_prompt_encoder`
- **æ–¹æ³•æè¿°**ï¼šè®¾ç½®æç¤ºç¼–ç å™¨ï¼ˆä»…é€‚ç”¨äºæç¤ºå­¦ä¹ æ–¹æ³•ï¼‰
- **ä¼ å…¥å‚æ•°**ï¼š
  - `adapter_name` (`str`): é€‚é…å™¨åç§°
- **è¿”å›å‚æ•°**ï¼šæ— 

#### method è§£è¯»
```python
# è·å–æŒ‡å®šé€‚é…å™¨çš„é…ç½®
config = self.peft_config[adapter_name]

# å¦‚æœæç¤ºç¼–ç å™¨æ¨¡å—ä¸å­˜åœ¨ï¼Œåˆ™åˆå§‹åŒ–æç¤ºç¼–ç å™¨å’Œæç¤ºä»¤ç‰Œå­—å…¸
if not hasattr(self, "prompt_encoder"):
    self.prompt_encoder = torch.nn.ModuleDict({})  # å­˜å‚¨ä¸åŒé€‚é…å™¨çš„æç¤ºç¼–ç å™¨
    self.prompt_tokens = {}  # å­˜å‚¨ä¸åŒé€‚é…å™¨çš„æç¤ºä»¤ç‰Œ

# åˆå§‹åŒ– transformer ä¸»å¹²æ¨¡å‹
transformer_backbone = None
# éå†åŸºç¡€æ¨¡å‹çš„ç›´æ¥å­æ¨¡å—
for name, module in self.base_model.named_children():
    # å†»ç»“æ‰€æœ‰å­æ¨¡å—çš„å‚æ•°ï¼Œåªè®­ç»ƒæç¤ºç›¸å…³å‚æ•°
    for param in module.parameters():
        param.requires_grad = False
    # å¦‚æœæ˜¯ PreTrainedModel å®ä¾‹ï¼Œå°†å…¶æ ‡è®°ä¸º transformer ä¸»å¹²
    if isinstance(module, PreTrainedModel):
        # Make sure to freeze Tranformers model
        if transformer_backbone is None:
            transformer_backbone = module
            self.transformer_backbone_name = name  # ä¿å­˜ä¸»å¹²æ¨¡å—åç§°

# å¦‚æœæ²¡æœ‰æ‰¾åˆ° transformer ä¸»å¹²ï¼Œåˆ™ä½¿ç”¨æ•´ä¸ªåŸºç¡€æ¨¡å‹
if transformer_backbone is None:
    transformer_backbone = self.base_model

# å¦‚æœæ²¡æœ‰æŒ‡å®š transformer å­æ¨¡å—æ•°é‡ï¼Œåˆ™æ ¹æ®ä»»åŠ¡ç±»å‹è®¾ç½®
if config.num_transformer_submodules is None:
    # SEQ_2_SEQ_LM ä»»åŠ¡ï¼ˆå¦‚ T5ï¼‰éœ€è¦ 2 ä¸ªå­æ¨¡å—ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨ï¼‰
    # å…¶ä»–ä»»åŠ¡åªéœ€è¦ 1 ä¸ªå­æ¨¡å—
    config.num_transformer_submodules = 2 if config.task_type == TaskType.SEQ_2_SEQ_LM else 1

# ç¡®å®šè¯åµŒå…¥å±‚çš„ä½ç½®
word_embeddings = None
try:
    # é¦–å…ˆå°è¯•é€šè¿‡æ ‡å‡†è·¯å¾„æ‰¾åˆ°è¯åµŒå…¥ï¼ˆé€‚ç”¨äº BERTã€RoBERTaã€DeBERTa ç­‰æ¨¡å‹ï¼‰
    word_embeddings = self.base_model.get_submodule("embeddings.word_embeddings")
except AttributeError:
    pass

# å¦‚æœé€šè¿‡æ ‡å‡†è·¯å¾„æ²¡æœ‰æ‰¾åˆ°è¯åµŒå…¥ï¼Œåˆ™é€šè¿‡å‚æ•°å¤§å°æ¨æ–­
if word_embeddings is None:
    # éå† transformer ä¸»å¹²çš„æ‰€æœ‰å‘½åå‚æ•°ï¼Œæ‰¾åˆ°ä¸è¯æ±‡è¡¨å¤§å°åŒ¹é…çš„å‚æ•°
    for named_param, value in list(transformer_backbone.named_parameters()):
        # å¤„ç† ZeRO-3 åˆ†å¸ƒå¼è®­ç»ƒæƒ…å†µï¼ŒDeepSpeed ä¼šå°†åˆ†ç‰‡å¼ é‡ä¿®æ”¹ä¸ºå½¢çŠ¶ [0]
        # å®é™…çš„æœªåˆ†ç‰‡å½¢çŠ¶å­˜å‚¨åœ¨ "ds_shape" å±æ€§ä¸­
        deepspeed_distributed_tensor_shape = getattr(value, "ds_shape", None)

        # å¤„ç†å¤šæ¨¡æ€æ¨¡å‹ï¼ˆVLMï¼‰çš„æƒ…å†µï¼Œè·å–æ–‡æœ¬é…ç½®ä¸­çš„è¯æ±‡è¡¨å¤§å°
        if hasattr(self.base_model.config, "get_text_config"):
            vocab_size = self.base_model.config.get_text_config().vocab_size
        # å…¼å®¹æ—§ç‰ˆæœ¬ transformers çš„å¤šæ¨¡æ€é…ç½®
        elif "text_config" in self.base_model.config:
            vocab_size = self.base_model.config.text_config.vocab_size
        else:
            vocab_size = self.base_model.config.vocab_size

        # æ£€æŸ¥å‚æ•°çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯å¦ç­‰äºè¯æ±‡è¡¨å¤§å°ï¼ˆè¯åµŒå…¥çŸ©é˜µçš„ç‰¹å¾ï¼‰
        if value.shape[0] == vocab_size or (
            deepspeed_distributed_tensor_shape is not None
            and deepspeed_distributed_tensor_shape[0] == vocab_size
        ):
            # è·å–è¯¥å‚æ•°å¯¹åº”çš„æ¨¡å—ï¼ˆå»æ‰ ".weight" åç¼€ï¼‰
            word_embeddings = transformer_backbone.get_submodule(named_param.replace(".weight", ""))
            break

# ä¿å­˜æ‰¾åˆ°çš„è¯åµŒå…¥æ¨¡å—
self.word_embeddings = word_embeddings

# æ ¹æ® PEFT ç±»å‹è·å–å¯¹åº”çš„è°ƒä¼˜å™¨ç±»
model_cls = PEFT_TYPE_TO_TUNER_MAPPING[config.peft_type]

# æ ¹æ®ä¸åŒçš„æç¤ºå­¦ä¹ ç±»å‹åˆ›å»ºç›¸åº”çš„æç¤ºç¼–ç å™¨
if config.peft_type in (PeftType.PROMPT_TUNING, PeftType.MULTITASK_PROMPT_TUNING, PeftType.CPT):
    # æç¤ºè°ƒä¼˜ã€å¤šä»»åŠ¡æç¤ºè°ƒä¼˜ã€CPTï¼šéœ€è¦è¯åµŒå…¥ä¿¡æ¯
    prompt_encoder = model_cls(config, self.word_embeddings)
elif config.peft_type == PeftType.P_TUNING:
    # P-Tuningï¼šåªéœ€è¦é…ç½®ä¿¡æ¯
    prompt_encoder = model_cls(config)
elif config.peft_type == PeftType.PREFIX_TUNING:
    # å‰ç¼€è°ƒä¼˜ï¼šéœ€è¦æ£€æŸ¥æ˜¯å¦ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹å…¼å®¹
    # prefix tuning ç°åœ¨ä½¿ç”¨ Cacheï¼Œä½†ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸å…¼å®¹
    if any(getattr(module, "gradient_checkpointing", False) for module in self.get_base_model().modules()):
        raise ValueError("Prefix tuning does not work with gradient checkpointing.")
    prompt_encoder = model_cls(config)
else:
    # ä¸æ”¯æŒçš„æç¤ºå­¦ä¹ ç±»å‹
    raise ValueError("Not supported")

# å°†æç¤ºç¼–ç å™¨ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Š
prompt_encoder = prompt_encoder.to(self.device)

# å°†æ–°åˆ›å»ºçš„æç¤ºç¼–ç å™¨æ·»åŠ åˆ° ModuleDict ä¸­
self.prompt_encoder.update(torch.nn.ModuleDict({adapter_name: prompt_encoder}))

# ä¸ºé€‚é…å™¨åˆ›å»ºæç¤ºä»¤ç‰Œå¼ é‡
# èŒƒå›´ï¼š0 åˆ° (è™šæ‹Ÿä»¤ç‰Œæ•° * transformer å­æ¨¡å—æ•° - 1)
self.prompt_tokens[adapter_name] = torch.arange(
    config.num_virtual_tokens * config.num_transformer_submodules
).long()
```

### `get_prompt_embedding_to_save`
- **æ–¹æ³•æè¿°**ï¼šè¿”å›ä¿å­˜æ¨¡å‹æ—¶è¦ä¿å­˜çš„æç¤ºåµŒå…¥ã€‚ä»…åœ¨ä½¿ç”¨æç¤ºå­¦ä¹ æ–¹æ³•æ—¶é€‚ç”¨
- **ä¼ å…¥å‚æ•°**ï¼š
  - `adapter_name` (`str`): é€‚é…å™¨åç§°
- **è¿”å›å‚æ•°**ï¼š`torch.Tensor` - æç¤ºåµŒå…¥å¼ é‡

### `get_prompt`
- **æ–¹æ³•æè¿°**ï¼šè¿”å›ç”¨äº Peft çš„è™šæ‹Ÿæç¤ºã€‚ä»…åœ¨ä½¿ç”¨æç¤ºå­¦ä¹ æ–¹æ³•æ—¶é€‚ç”¨
- **ä¼ å…¥å‚æ•°**ï¼š
  - `batch_size` (`int`): æ‰¹æ¬¡å¤§å°
  - `task_ids` (`torch.Tensor`, *å¯é€‰*): ä»»åŠ¡ ID
  - `max_cache_len` (`int`, *å¯é€‰*): æœ€å¤§ç¼“å­˜é•¿åº¦
- **è¿”å›å‚æ•°**ï¼š`torch.Tensor` - è™šæ‹Ÿæç¤ºå¼ é‡

#### method è§£è¯»
```python
# è·å–å½“å‰æ´»åŠ¨é€‚é…å™¨çš„é…ç½®å’Œæç¤ºç¼–ç å™¨
peft_config = self.active_peft_config
prompt_encoder = self.prompt_encoder[self.active_adapter]

# å‡†å¤‡æç¤ºä»¤ç‰Œå¼ é‡ï¼šæ‰©å±•åˆ°æŒ‡å®šçš„æ‰¹æ¬¡å¤§å°å¹¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
prompt_tokens = (
    self.prompt_tokens[self.active_adapter]
    .unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    .expand(batch_size, -1)  # æ‰©å±•åˆ°æŒ‡å®šçš„æ‰¹æ¬¡å¤§å°
    .to(prompt_encoder.embedding.weight.device)  # ç§»åŠ¨åˆ°ç¼–ç å™¨æƒé‡æ‰€åœ¨çš„è®¾å¤‡
)

# æ ¹æ®ä¸åŒçš„æç¤ºå­¦ä¹ ç±»å‹ç”Ÿæˆæç¤º
if peft_config.peft_type == PeftType.PREFIX_TUNING:
    # å‰ç¼€è°ƒä¼˜ï¼šç”Ÿæˆ past_key_values ç”¨äºæ³¨æ„åŠ›æœºåˆ¶
    # åªä½¿ç”¨å‰ n ä¸ªè™šæ‹Ÿä»¤ç‰Œ
    prompt_tokens = prompt_tokens[:, : peft_config.num_virtual_tokens]

    if peft_config.inference_mode:
        # æ¨ç†æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ç¼–ç å™¨æƒé‡ï¼Œä¸è¿›è¡Œå‰å‘ä¼ æ’­
        past_key_values = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)
    else:
        # è®­ç»ƒæ¨¡å¼ï¼šé€šè¿‡ç¼–ç å™¨å‰å‘ä¼ æ’­ç”Ÿæˆæç¤º
        past_key_values = prompt_encoder(prompt_tokens)

    # è½¬æ¢æ•°æ®ç±»å‹ä»¥åŒ¹é…åŸºç¡€æ¨¡å‹
    if self.base_model_torch_dtype is not None:
        past_key_values = past_key_values.to(self.base_model_torch_dtype)

    # é‡å¡‘å¼ é‡ä»¥é€‚åº”æ³¨æ„åŠ›æœºåˆ¶çš„ç»“æ„
    # [batch_size, num_virtual_tokens, num_layers*2, num_heads, head_dim]
    past_key_values = past_key_values.view(
        batch_size,
        peft_config.num_virtual_tokens,
        peft_config.num_layers * 2,  # *2 å› ä¸ºæœ‰ key å’Œ value
        peft_config.num_attention_heads,
        peft_config.token_dim // peft_config.num_attention_heads,  # head_dim
    )

    # å¯¹äºç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå¤åˆ¶ä¸€ä»½ç”¨äºè§£ç å™¨
    if peft_config.num_transformer_submodules == 2:
        past_key_values = torch.cat([past_key_values, past_key_values], dim=2)

    # é‡æ–°æ’åˆ—ç»´åº¦ï¼š[num_layers*2, batch_size, num_heads, num_virtual_tokens, head_dim]
    # ç„¶ååˆ†å‰²æˆç¼–ç å™¨å’Œè§£ç å™¨çš„ç¼“å­˜
    past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(
        peft_config.num_transformer_submodules * 2
    )

    # è·å–åŸºç¡€æ¨¡å‹é…ç½®ä»¥è¿›è¡Œåå¤„ç†
    base_model = self.get_base_model()
    model_config = getattr(base_model, "config", None)
    model_type = getattr(model_config, "model_type", "")

    # æ ¹æ®æ¨¡å‹ç±»å‹åº”ç”¨ç‰¹å®šçš„åå¤„ç†å‡½æ•°
    if TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING.get(self.config.model_type, None) is not None:
        # ä½¿ç”¨æ¨¡å‹ç‰¹å®šçš„åå¤„ç†å‡½æ•°
        post_process_fn = TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING[self.config.model_type]
        past_key_values = post_process_fn(past_key_values)
    elif ("gemma2" in model_type) or ("gemma3_text" in model_type):
        # Gemma2 å’Œ Gemma3 ç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ HybridCache
        if max_cache_len is None:
            raise ValueError(
                "max_cache_len is None but it should have been passed. Something went wrong, please open an "
                "issue on GitHub with a reproducer: https://github.com/huggingface/peft/issues"
            )
        base_config = base_model.config
        if hasattr(base_config, "get_text_config"):
            base_config = base_config.get_text_config()

        # åˆ›å»º HybridCache å®ä¾‹
        new_cache = HybridCache(
            base_config,
            max_batch_size=batch_size,
            max_cache_len=max_cache_len,
            dtype=past_key_values[0].dtype,
            device=past_key_values[0].device,
        )

        # æ›´æ–°ç¼“å­˜ä¸­çš„é”®å€¼å¯¹
        cache_position = torch.arange(peft_config.num_virtual_tokens, device=past_key_values[0].device)
        for layer_idx in range(peft_config.num_layers):
            key_states, value_states = past_key_values[0][layer_idx], past_key_values[1][layer_idx]
            new_cache.update(
                key_states, value_states, layer_idx, cache_kwargs={"cache_position": cache_position}
            )
        past_key_values = new_cache
    elif peft_config.num_transformer_submodules == 1:
        # å•æ¨¡å—æ¨¡å‹ï¼šä½¿ç”¨ DynamicCache
        past_key_values = DynamicCache.from_legacy_cache(past_key_values)
    elif (peft_config.num_transformer_submodules == 2) and getattr(
        self.base_model, "_supports_cache_class", True
    ):
        # ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼šä½¿ç”¨ EncoderDecoderCache
        past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)
        past_key_values.cross_attention_cache = DynamicCache()
        past_key_values.is_updated = {
            layer_idx: False for layer_idx in range(len(past_key_values.cross_attention_cache.key_cache))
        }

    # ç¡®ä¿ç¼“å­˜å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    map_cache_to_layer_device_map(self.get_base_model(), past_key_values)
    return past_key_values
else:
    # å…¶ä»–æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆPrompt Tuning, P-Tuning, Multitask Prompt Tuning ç­‰ï¼‰
    if peft_config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:
        # å¤šä»»åŠ¡æç¤ºè°ƒä¼˜ï¼šéœ€è¦ä»»åŠ¡ ID
        prompts = prompt_encoder(prompt_tokens, task_ids)
    else:
        # å•ä»»åŠ¡æç¤ºè°ƒä¼˜
        if peft_config.inference_mode:
            # æ¨ç†æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ç¼–ç å™¨æƒé‡
            prompts = prompt_encoder.embedding.weight
        else:
            # è®­ç»ƒæ¨¡å¼ï¼šä¼˜åŒ–ç­–ç•¥ - åªå¤„ç†ä¸€ä¸ªæ ·æœ¬ç„¶åé‡å¤è¾“å‡º
            # è¿™æ˜¯ä¸ºäº†æé«˜æ•ˆç‡ï¼Œé¿å…é‡å¤è®¡ç®—ç›¸åŒçš„ç¼–ç ç»“æœ
            # å‚è€ƒ: https://github.com/huggingface/peft/issues/2043#issuecomment-2321522577
            prompt_tokens = prompt_tokens[:1]  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ä»¤ç‰Œ
            prompts = prompt_encoder(prompt_tokens)  # ç¼–ç ä¸€æ¬¡

        # é‡å¤ç¼–ç ç»“æœä»¥åŒ¹é…æ‰¹æ¬¡å¤§å°
        prompts = prompts.repeat(batch_size, 1, 1)
    return prompts
```

## å†…éƒ¨å’Œç‰¹æ®Šæ–¹æ³•

### `__getattr__`
- **æ–¹æ³•æè¿°**ï¼šå°†ç¼ºå¤±å±æ€§è½¬å‘åˆ°åŒ…è£…æ¨¡å—
- **ä¼ å…¥å‚æ•°**ï¼š
  - `name` (`str`): å±æ€§åç§°
- **è¿”å›å‚æ•°**ï¼šå±æ€§å€¼

### `_enable_peft_forward_hooks`
- **æ–¹æ³•æè¿°**ï¼šå¯ç”¨ PEFT å‰å‘é’©å­çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
- **ä¼ å…¥å‚æ•°**ï¼š
  - `*args`: ä½ç½®å‚æ•°
  - `**kwargs`: å…³é”®å­—å‚æ•°
- **è¿”å›å‚æ•°**ï¼šä¸Šä¸‹æ–‡ç®¡ç†å™¨

### `_split_kwargs`
- **æ–¹æ³•æè¿°**ï¼šåˆ†å‰² kwargs çš„ç±»æ–¹æ³•
- **ä¼ å…¥å‚æ•°**ï¼š
  - `kwargs` (`dict[str, Any]`): è¦åˆ†å‰²çš„ kwargs
- **è¿”å›å‚æ•°**ï¼š`tuple` - (hf_hub_download_kwargs, other_kwargs)

### `_update_offload`
- **æ–¹æ³•æè¿°**ï¼šæ›´æ–°ç£ç›˜å¸è½½æ¨¡å—çš„ offload_index å’Œ safetensors æ–‡ä»¶ï¼Œç”¨äºåŠ è½½å’Œåˆå¹¶ PeftModels
- **ä¼ å…¥å‚æ•°**ï¼š
  - `offload_index` (`dict[str, dict[str, str]]`): ç£ç›˜å¸è½½æ¨¡å—çš„å­—å…¸ï¼ŒåŒ…å«å…¶å…ƒæ•°æ®å’Œ safetensors æ–‡ä»¶å
  - `adapters_weights` (`dict[str, torch.tensor]`): Peft é€‚é…å™¨æ¨¡å—åç§°å’Œæƒé‡çš„å­—å…¸
- **è¿”å›å‚æ•°**ï¼šæ›´æ–°åçš„ offload_index

### `_check_new_adapter_config`
- **æ–¹æ³•æè¿°**ï¼šå¯¹æ–°æ·»åŠ çš„ PEFT é…ç½®æ‰§è¡Œæ£€æŸ¥ä»¥ç¡®ä¿å®Œæ•´æ€§
- **ä¼ å…¥å‚æ•°**ï¼š
  - `peft_config` (`PeftConfig`): è¦æ£€æŸ¥çš„ PEFT é…ç½®
  - `is_trainable` (`bool`): æ˜¯å¦å¯è®­ç»ƒ
- **è¿”å›å‚æ•°**ï¼šæ— 